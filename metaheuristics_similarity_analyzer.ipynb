{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "import niapy\n",
    "from niapy.algorithms.basic import (\n",
    "    BatAlgorithm,\n",
    "    FireflyAlgorithm,\n",
    "    ParticleSwarmOptimization,\n",
    ")\n",
    "from niapy.algorithms import Algorithm\n",
    "from niapy.problems import Problem\n",
    "from niapy.problems.sphere import Sphere\n",
    "from niapy.task import Task\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import torchinfo\n",
    "from PIL import Image\n",
    "import pandas\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from util.optimization_data import SingleRunData, PopulationData\n",
    "from util.diversity_metrics import DiversityMetric\n",
    "from util.constants import RNG_SEED, DATASET_PATH, BATCH_SIZE, EPOCHS, MAX_ITER, NUM_RUNS\n",
    "\n",
    "execute_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(algorithm: Algorithm, task: Task, single_run_data: SingleRunData):\n",
    "    r\"\"\"An adaptation of NiaPy Algorithm run method.\n",
    "\n",
    "    Args:\n",
    "        algorithm (Algorithm): Algorithm.\n",
    "        task (Task): Task with pre configured parameters.\n",
    "        single_run_data (SingleRunData): Instance for archiving optimization results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        algorithm.callbacks.before_run()\n",
    "        algorithm.rng = default_rng(seed=RNG_SEED)\n",
    "        pop, fpop, params = algorithm.init_population(task)\n",
    "        # reset seed to random\n",
    "        algorithm.rng = default_rng()\n",
    "        xb, fxb = algorithm.get_best(pop, fpop)\n",
    "        while not task.stopping_condition():\n",
    "            algorithm.callbacks.before_iteration(pop, fpop, xb, fxb, **params)\n",
    "            pop, fpop, xb, fxb, params = algorithm.run_iteration(\n",
    "                task, pop, fpop, xb, fxb, **params\n",
    "            )\n",
    "\n",
    "            # save population data\n",
    "            pop_data = PopulationData(\n",
    "                population=np.array(pop), population_fitness=np.array(fpop)\n",
    "            )\n",
    "            pop_data.calculate_metrics(\n",
    "                [\n",
    "                    DiversityMetric.PDC,\n",
    "                    DiversityMetric.PED,\n",
    "                    DiversityMetric.PMD,\n",
    "                    DiversityMetric.AAD,\n",
    "                    DiversityMetric.PDI,\n",
    "                ],\n",
    "                task.problem,\n",
    "            )\n",
    "            single_run_data.add_population(pop_data)\n",
    "\n",
    "            algorithm.callbacks.after_iteration(pop, fpop, xb, fxb, **params)\n",
    "            task.next_iter()\n",
    "        algorithm.callbacks.after_run()\n",
    "        return xb, fxb * task.optimization_type.value\n",
    "    except BaseException as e:\n",
    "        if (\n",
    "            threading.current_thread() is threading.main_thread()\n",
    "            and multiprocessing.current_process().name == \"MainProcess\"\n",
    "        ):\n",
    "            raise e\n",
    "        algorithm.exception = e\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_runner(\n",
    "    algorithm: Algorithm, problem: Problem, max_iter: int, runs: int\n",
    "):\n",
    "    r\"\"\"An adaptation of NiaPy ALgorithm run method.\n",
    "\n",
    "    Args:\n",
    "        algorithm (Algorithm): Algorithm.\n",
    "        problem (Problem): Optimization problem.\n",
    "        max_iter (int): Optimization stopping condition.\n",
    "        runs (int): Number of runs to execute.\n",
    "    \"\"\"\n",
    "    for r in tqdm(range(runs)):\n",
    "        task = Task(problem, max_iters=max_iter)\n",
    "\n",
    "        single_run_data = SingleRunData(\n",
    "            algorithm_name=algorithm.Name,\n",
    "            algorithm_parameters=algorithm.get_parameters(),\n",
    "            problem_name=problem.name(),\n",
    "            max_iters=max_iter,\n",
    "        )\n",
    "\n",
    "        results = optimization(algorithm, task, single_run_data)\n",
    "        single_run_data.export_to_json(\n",
    "            os.path.join(\n",
    "                DATASET_PATH, algorithm.Name[0], problem.name(), f\"run_{r:05d}.json\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in [BatAlgorithm(), FireflyAlgorithm(), ParticleSwarmOptimization()]:\n",
    "    sphere = Sphere()\n",
    "    #optimization_runner(algorithm, sphere, MAX_ITER, NUM_RUNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path_list, classes) -> None:\n",
    "        super().__init__()\n",
    "        self.data_path_list = data_path_list\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        run = SingleRunData.import_from_json(self.data_path_list[index])\n",
    "        metrics = run.get_population_metrics()\n",
    "        # normalize values\n",
    "        sklearn.preprocessing.minmax_scale(metrics, feature_range=(0, 1), axis=0, copy=True)\n",
    "        one_hot = np.zeros_like(self.classes, float)\n",
    "        one_hot[self.classes.index(run.algorithm_name[0])] = 1\n",
    "\n",
    "        return torch.tensor(metrics), torch.tensor(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_len, num_classes, num_hidden=256, num_layers=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_len,\n",
    "            hidden_size=num_hidden,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.8\n",
    "        )\n",
    "        self.fc = nn.Linear(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        features = hidden[-1]\n",
    "        out = self.fc(features)\n",
    "\n",
    "        return features, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = []\n",
    "classes = []\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    classes.append(algorithm)\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        for run in os.listdir(os.path.join(DATASET_PATH, algorithm, problem)):\n",
    "            dataset_paths.append(os.path.join(DATASET_PATH, algorithm, problem, run))\n",
    "\n",
    "x_train, x_test = sklearn.model_selection.train_test_split(dataset_paths, test_size=0.2, shuffle=True, random_state=42)\n",
    "x_train, x_val = sklearn.model_selection.train_test_split(x_train, test_size=0.25, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_generator(x_train, classes)\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "val_dataset = data_generator(x_val, classes)\n",
    "val_data_loader = data.DataLoader(val_dataset, batch_size=1, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "test_dataset = data_generator(x_test, classes)\n",
    "test_data_loader = data.DataLoader(test_dataset, batch_size=1, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "input, target = next(iter(train_data_loader))\n",
    "print(np.shape(input))\n",
    "print(np.shape(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(model, train_data_loader, val_data_loader, epochs, loss_fn, optimizer, device, model_file_name):\n",
    "    loss_values = []\n",
    "    val_loss_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0.0\n",
    "        batch_count = 0\n",
    "        val_loss_sum = 0.0\n",
    "        val_batch_count = 0\n",
    "            \n",
    "        model.train()\n",
    "        for batch in train_data_loader:\n",
    "            input, target = batch\n",
    "            \n",
    "            target = target.to(device)\n",
    "            input = input.to(device)\n",
    "\n",
    "            _, pred = model(input)\n",
    "            loss = loss_fn(pred, target.argmax(1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_data_loader:\n",
    "                input, target = batch\n",
    "                \n",
    "                target = target.to(device)\n",
    "                input = input.to(device)\n",
    "\n",
    "                _, pred = model(input)\n",
    "                loss = loss_fn(pred, target.argmax(1))\n",
    "\n",
    "                val_loss_sum += loss.item()\n",
    "                val_batch_count += 1\n",
    "\n",
    "        loss_values.append(loss_sum/batch_count)\n",
    "        val_loss_values.append(val_loss_sum/val_batch_count)\n",
    "        print(f\"epoch: {epoch + 1}, loss: {loss_sum/batch_count :.10f}, val_loss: {val_loss_sum/val_batch_count :.10f}\")\n",
    "\n",
    "        torch.save(model, model_file_name)\n",
    "\n",
    "    x = [*range(1, epochs+1)]\n",
    "    plt.plot(x, loss_values, label=\"train loss\")\n",
    "    plt.plot(x, val_loss_values, label=\"val loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(NUM_RUNS, len(classes))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_file_name = f\"lstm_model.pt\"\n",
    "\n",
    "print(torchinfo.summary(model, (100, 3), depth=5))\n",
    "\n",
    "if execute_training:\n",
    "    model.to(device)\n",
    "    nn_train(model, train_data_loader, val_data_loader, EPOCHS, loss_fn, optimizer, device, model_file_name)\n",
    "    torch.save(model, model_file_name)\n",
    "else:\n",
    "    model = torch.load(model_file_name, map_location=torch.device(device))\n",
    "    model.to(device)\n",
    "    loss_plot = np.asarray(Image.open('loss_plot.png'))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(loss_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
