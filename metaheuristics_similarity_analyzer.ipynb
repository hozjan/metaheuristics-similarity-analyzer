{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niapy.algorithms.basic import (\n",
    "    BatAlgorithm,\n",
    "    ParticleSwarmAlgorithm,\n",
    "    ParticleSwarmOptimization,\n",
    ")\n",
    "from tools.algorithms.fa import FireflyAlgorithm\n",
    "from niapy.problems.schwefel import Schwefel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sklearn\n",
    "from scipy import spatial, stats\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pygad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "from tools.ml_tools import get_data_loaders, nn_test, nn_train, NNType, LSTMClassifier, LinearClassifier\n",
    "from util.optimization_data import SingleRunData\n",
    "from util.pop_diversity_metrics import PopDiversityMetric\n",
    "from tools.optimization_tools import optimization_runner\n",
    "from tools.metaheuristic_similarity_analyzer import MetaheuristicSimilarityAnalyzer\n",
    "\n",
    "from util.constants import (\n",
    "    RNG_SEED,\n",
    "    BATCH_SIZE,\n",
    "    DATASET_PATH,\n",
    "    EPOCHS,\n",
    "    POP_SIZE,\n",
    "    MAX_EVALS,\n",
    "    NUM_RUNS,\n",
    "    OPTIMIZATION_PROBLEM,\n",
    "    META_GA_CROSSOVER_PROBABILITY,\n",
    "    META_GA_CROSSOVER_TYPE,\n",
    "    META_GA_GENERATIONS,\n",
    "    META_GA_K_TOURNAMENT,\n",
    "    META_GA_KEEP_ELITISM,\n",
    "    META_GA_MUTATION_NUM_GENES,\n",
    "    META_GA_MUTATION_TYPE,\n",
    "    META_GA_PARENT_SELECTION_TYPE,\n",
    "    META_GA_PERCENT_PARENTS_MATING,\n",
    "    META_GA_SOLUTIONS_PER_POP,\n",
    "    GENE_SPACES,\n",
    "    TARGET_GENE_SPACES,\n",
    "    POP_DIVERSITY_METRICS,\n",
    "    INDIV_DIVERSITY_METRICS,\n",
    "    N_PCA_COMPONENTS,\n",
    "    LSTM_NUM_LAYERS,\n",
    "    LSTM_HIDDEN_DIM,\n",
    "    LSTM_DROPOUT,\n",
    "    VAL_SIZE,\n",
    "    TEST_SIZE,\n",
    ")\n",
    "\n",
    "BASE_PATH = \"./archive/target_performance_similarity/01-29_15.27.22_WVCPSO_Schwefel\"\n",
    "_DATASET_PATH = f\"{BASE_PATH}/dataset\"\n",
    "DATASET_PATH = f\"{_DATASET_PATH}/0_subset\"\n",
    "MSA_PATH = f\"{BASE_PATH}/msa_obj\"\n",
    "\n",
    "algorithms_to_plot = ['FA', 'WVCPSO', 'PSO', 'BA']\n",
    "\n",
    "execute_training = True\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "print(\"CPUs: \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness values by subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "mean_fitness_1 = []\n",
    "min_fitness_1 = []\n",
    "std_fitness_1 = []\n",
    "\n",
    "mean_fitness_2 = []\n",
    "min_fitness_2 = []\n",
    "std_fitness_2 = []\n",
    "\n",
    "wilcoxon = []\n",
    "\n",
    "al1 = \"\"\n",
    "al2 = \"\"\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    best_al1 = []\n",
    "    best_al2 = []\n",
    "    subset = f\"{idx}_subset\"\n",
    "    for idx, algorithm in enumerate(os.listdir(os.path.join(dataset_path, subset))):\n",
    "        best_fitness_values = []\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            for run in runs:\n",
    "                run_path = os.path.join(dataset_path, subset, algorithm, problem, run)\n",
    "                srd = SingleRunData.import_from_json(run_path)\n",
    "                best_fitness_values.append(srd.best_fitness)\n",
    "        if idx == 0:\n",
    "            al1 = algorithm\n",
    "            mean_fitness_1.append(round(np.mean(best_fitness_values), 2))\n",
    "            min_fitness_1.append(round(np.amin(best_fitness_values), 2))\n",
    "            std_fitness_1.append(round(np.std(best_fitness_values), 2))\n",
    "            best_al1 = best_fitness_values\n",
    "        else:\n",
    "            al2 = algorithm\n",
    "            mean_fitness_2.append(round(np.mean(best_fitness_values), 2))\n",
    "            min_fitness_2.append(round(np.amin(best_fitness_values), 2))\n",
    "            std_fitness_2.append(round(np.std(best_fitness_values), 2))\n",
    "            best_al2 = best_fitness_values\n",
    "\n",
    "    shapiro_1 = stats.shapiro(best_al1)\n",
    "    shapiro_2 = stats.shapiro(best_al2)\n",
    "    if shapiro_1[1] > 0.05 and shapiro_2[1] > 0.05:\n",
    "        stats_levene = stats.levene(best_al1, best_al2)\n",
    "        if stats_levene[1] > 0.05:\n",
    "            print(f\"obe normalno, varianci enaki\")\n",
    "        else:\n",
    "            print(f\"obe normalno, različni\")\n",
    "    else:\n",
    "        print(\"ne\")\n",
    "\n",
    "    data = stats.wilcoxon(best_al1, best_al2)\n",
    "    wilcoxon.append(data[1])\n",
    "\n",
    "print(al1)\n",
    "for min, mean, std in zip(min_fitness_1, mean_fitness_1, std_fitness_1):\n",
    "    print(f\"{min} & {mean} & {std}\")\n",
    "\n",
    "print(f\"\\n & {round(np.min(min_fitness_1), 2)} & {round(np.min(mean_fitness_1), 2)} & {round(np.min(std_fitness_1), 2)}\")\n",
    "print(f\" & {round(np.mean(min_fitness_1), 2)} & {round(np.mean(mean_fitness_1), 2)} & {round(np.mean(std_fitness_1), 2)}\")\n",
    "print(f\" & {round(np.max(min_fitness_1), 2)} & {round(np.max(mean_fitness_1), 2)} & {round(np.max(std_fitness_1), 2)}\")\n",
    "print(f\" & {round(np.std(min_fitness_1), 2)} & {round(np.std(mean_fitness_1), 2)} & {round(np.std(std_fitness_1), 2)}\")\n",
    "\n",
    "print(\"\\n\", al2)\n",
    "for min, mean, std in zip(min_fitness_2, mean_fitness_2, std_fitness_2):\n",
    "    print(f\"{min} & {mean} & {std}\")\n",
    "\n",
    "print(f\"\\n & {round(np.min(min_fitness_2), 2)} & {round(np.min(mean_fitness_2), 2)} & {round(np.min(std_fitness_2), 2)}\")\n",
    "print(f\" & {round(np.mean(min_fitness_2), 2)} & {round(np.mean(mean_fitness_2), 2)} & {round(np.mean(std_fitness_2), 2)}\")\n",
    "print(f\" & {round(np.max(min_fitness_2), 2)} & {round(np.max(mean_fitness_2), 2)} & {round(np.max(std_fitness_2), 2)}\")\n",
    "print(f\" & {round(np.std(min_fitness_2), 2)} & {round(np.std(mean_fitness_2), 2)} & {round(np.std(std_fitness_2), 2)}\")\n",
    "\n",
    "for w in wilcoxon:\n",
    "    if w < 0.05:\n",
    "        print(\"Različna\")\n",
    "    else:\n",
    "        print(\"Enaka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "mean_fitness_1 = []\n",
    "min_fitness_1 = []\n",
    "std_fitness_1 = []\n",
    "\n",
    "mean_fitness_2 = []\n",
    "min_fitness_2 = []\n",
    "std_fitness_2 = []\n",
    "\n",
    "wilcoxon = []\n",
    "\n",
    "al1 = \"\"\n",
    "al2 = \"\"\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    best_al1 = []\n",
    "    best_al2 = []\n",
    "    subset = f\"{idx}_subset\"\n",
    "    for idx, algorithm in enumerate(os.listdir(os.path.join(dataset_path, subset))):\n",
    "        best_fitness_values = []\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            for run in runs:\n",
    "                run_path = os.path.join(dataset_path, subset, algorithm, problem, run)\n",
    "                srd = SingleRunData.import_from_json(run_path)\n",
    "                best_fitness_values.append(srd.best_fitness)\n",
    "        if idx == 0:\n",
    "            al1 = algorithm\n",
    "            mean_fitness_1.append(round(np.mean(best_fitness_values), 2))\n",
    "            min_fitness_1.append(round(np.amin(best_fitness_values), 2))\n",
    "            std_fitness_1.append(round(np.std(best_fitness_values), 2))\n",
    "            best_al1 = best_fitness_values\n",
    "        else:\n",
    "            al2 = algorithm\n",
    "            mean_fitness_2.append(round(np.mean(best_fitness_values), 2))\n",
    "            min_fitness_2.append(round(np.amin(best_fitness_values), 2))\n",
    "            std_fitness_2.append(round(np.std(best_fitness_values), 2))\n",
    "            best_al2 = best_fitness_values\n",
    "\n",
    "\n",
    "print(al1)\n",
    "for min, mean, std in zip(min_fitness_1, mean_fitness_1, std_fitness_1):\n",
    "    line = \"\"\n",
    "    if min == np.min(min_fitness_1):\n",
    "        line += \"\\\\textbf{\" + f\"{min}\" + \"} & \"\n",
    "    else:\n",
    "        line += f\"{min} & \"\n",
    "    \n",
    "    if mean == np.min(mean_fitness_1):\n",
    "        line += \"\\\\textbf{\" + f\"{mean}\" + \"} & \"\n",
    "    else:\n",
    "        line += f\"{mean} & \"\n",
    "\n",
    "    if std == np.min(std_fitness_1):\n",
    "        line += \"\\\\textbf{\" + f\"{std}\" + \"}\"\n",
    "    else:\n",
    "        line += f\"{std}\"\n",
    "    print(line)\n",
    "\n",
    "print(f\"\\n & {round(np.min(min_fitness_1), 2)} & {round(np.min(mean_fitness_1), 2)} & {round(np.min(std_fitness_1), 2)}\")\n",
    "print(f\" & {round(np.mean(min_fitness_1), 2)} & {round(np.mean(mean_fitness_1), 2)} & {round(np.mean(std_fitness_1), 2)}\")\n",
    "print(f\" & {round(np.max(min_fitness_1), 2)} & {round(np.max(mean_fitness_1), 2)} & {round(np.max(std_fitness_1), 2)}\")\n",
    "print(f\" & {round(np.std(min_fitness_1), 2)} & {round(np.std(mean_fitness_1), 2)} & {round(np.std(std_fitness_1), 2)}\")\n",
    "\n",
    "print(\"\\n\", al2)\n",
    "for min, mean, std in zip(min_fitness_2, mean_fitness_2, std_fitness_2):\n",
    "    line = \"\"\n",
    "    if min == np.min(min_fitness_2):\n",
    "        line += \"\\\\textbf{\" + f\"{min}\" + \"} & \"\n",
    "    else:\n",
    "        line += f\"{min} & \"\n",
    "    \n",
    "    if mean == np.min(mean_fitness_2):\n",
    "        line += \"\\\\textbf{\" + f\"{mean}\" + \"} & \"\n",
    "    else:\n",
    "        line += f\"{mean} & \"\n",
    "\n",
    "    if std == np.min(std_fitness_2):\n",
    "        line += \"\\\\textbf{\" + f\"{std}\" + \"}\"\n",
    "    else:\n",
    "        line += f\"{std}\"\n",
    "    print(line)\n",
    "\n",
    "print(f\"\\n & {round(np.min(min_fitness_2), 2)} & {round(np.min(mean_fitness_2), 2)} & {round(np.min(std_fitness_2), 2)}\")\n",
    "print(f\" & {round(np.mean(min_fitness_2), 2)} & {round(np.mean(mean_fitness_2), 2)} & {round(np.mean(std_fitness_2), 2)}\")\n",
    "print(f\" & {round(np.max(min_fitness_2), 2)} & {round(np.max(mean_fitness_2), 2)} & {round(np.max(std_fitness_2), 2)}\")\n",
    "print(f\" & {round(np.std(min_fitness_2), 2)} & {round(np.std(mean_fitness_2), 2)} & {round(np.std(std_fitness_2), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = MetaheuristicSimilarityAnalyzer.import_from_pkl(MSA_PATH)\n",
    "analyzer.msa_info()\n",
    "print(analyzer.similarity, \"\\n\")\n",
    "for i, solution in enumerate(analyzer.optimized_solutions):\n",
    "    print(f\"{i} {np.round(solution, 2)}\")\n",
    "print(\"\")\n",
    "for i, solution in enumerate(analyzer.target_solutions):\n",
    "    print(f\"{i} {np.round(solution, 2)}\")\n",
    "\n",
    "\n",
    "print(\"min: \", \" & \".join(map(str, np.round(np.min(np.array(analyzer.target_solutions), axis=0), 2))), \"&\", \" & \".join(map(str, np.round(np.min(np.array(analyzer.optimized_solutions), axis=0), 2))))\n",
    "print(\"avg: \", \" & \".join(map(str, np.round(np.mean(np.array(analyzer.target_solutions), axis=0), 2))), \"&\", \" & \".join(map(str, np.round(np.mean(np.array(analyzer.optimized_solutions), axis=0), 2))))\n",
    "print(\"max: \", \" & \".join(map(str, np.round(np.max(np.array(analyzer.target_solutions), axis=0), 2))), \"&\", \" & \".join(map(str, np.round(np.max(np.array(analyzer.optimized_solutions), axis=0), 2))))\n",
    "print(\"std: \", \" & \".join(map(str, np.round(np.std(np.array(analyzer.target_solutions), axis=0), 2))), \"&\", \" & \".join(map(str, np.round(np.std(np.array(analyzer.optimized_solutions), axis=0), 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = MetaheuristicSimilarityAnalyzer.import_from_pkl(MSA_PATH)\n",
    "\n",
    "for target, optimized in zip(analyzer.target_solutions, analyzer.optimized_solutions):\n",
    "    line = \"\"\n",
    "    for t in target:\n",
    "        line += f\"{round(t, 2)} & \"\n",
    "    for idx, o in enumerate(optimized):\n",
    "        if idx == len(optimized) - 1:\n",
    "            line += f\"{round(o, 2)}\"\n",
    "        else:\n",
    "            line += f\"{round(o, 2)} & \"\n",
    "    print(line)\n",
    "\n",
    "print(\"\")\n",
    "print(\"min: \", \" & \".join(map(str, np.round(np.min(np.array(analyzer.target_solutions), axis=0), 2))), \"&\", \" & \".join(map(str, np.round(np.min(np.array(analyzer.optimized_solutions), axis=0), 2))))\n",
    "print(\"avg: \", \" & \".join(map(str, np.round(np.mean(np.array(analyzer.target_solutions), axis=0), 2))), \"&\", \" & \".join(map(str, np.round(np.mean(np.array(analyzer.optimized_solutions), axis=0), 2))))\n",
    "print(\"max: \", \" & \".join(map(str, np.round(np.max(np.array(analyzer.target_solutions), axis=0), 2))), \"&\", \" & \".join(map(str, np.round(np.max(np.array(analyzer.optimized_solutions), axis=0), 2))))\n",
    "print(\"std: \", \" & \".join(map(str, np.round(np.std(np.array(analyzer.target_solutions), axis=0), 2))), \"&\", \" & \".join(map(str, np.round(np.std(np.array(analyzer.optimized_solutions), axis=0), 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_instance_0 = pygad.load(\"./archive/target_performance_similarity/09-17_15.37.29_WVCPSO_Schwefel/1_FA_Schwefel/meta_ga_obj\")\n",
    "ga_instance_0.plot_genes(solutions=\"best\")\n",
    "ga_instance_0.plot_genes(solutions=\"all\")\n",
    "ga_instance_0.plot_new_solution_rate()\n",
    "\n",
    "print(ga_instance_0.best_solutions[-1])\n",
    "print(ga_instance_0.best_solutions_fitness[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_test_setting = True\n",
    "\n",
    "problem = OPTIMIZATION_PROBLEM\n",
    "\n",
    "if use_test_setting:\n",
    "    problem = Schwefel(dimension=20)\n",
    "    algorithms = [\n",
    "        #FireflyAlgorithm(population_size=POP_SIZE, alpha=0.15, beta0=0.4, gamma=0.04, theta=0.98),\n",
    "        FireflyAlgorithm(population_size=POP_SIZE, alpha=0.84464886, beta0=0.74171366, gamma=0.60686203, theta=0.97758844),\n",
    "        #FireflyAlgorithm(population_size=POP_SIZE, alpha=0.02, beta0=0.43, gamma=0.693, theta=0.962),\n",
    "        #ParticleSwarmAlgorithm(population_size=POP_SIZE, c1=1.14, c2=0.05, w=0.54),\n",
    "        #ParticleSwarmAlgorithm(population_size=POP_SIZE, c1=2.00417841, c2=0.70674774, w=0.82266951),\n",
    "        #ParticleSwarmAlgorithm(population_size=POP_SIZE, c1=1.95, c2=0.82, w=0.82),\n",
    "        #BatAlgorithm(population_size=POP_SIZE, loudness=1.0, pulse_rate=1.0, alpha=0.99, gamma=0.1)\n",
    "        #tools.algorithms.pso.ParticleSwarmAlgorithm(population_size=POP_SIZE, c1=1.95, c2=0.82, w=0.82),\n",
    "    ]\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    optimization_runner(\n",
    "        algorithm=algorithm,\n",
    "        problem=problem,\n",
    "        runs=1,\n",
    "        dataset_path=\"./dataset\",\n",
    "        pop_diversity_metrics=POP_DIVERSITY_METRICS,\n",
    "        indiv_diversity_metrics=INDIV_DIVERSITY_METRICS,\n",
    "        max_evals=MAX_EVALS,\n",
    "        run_index_seed=True,\n",
    "        keep_pop_data=False,\n",
    "        parallel_processing=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population diversity metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_metrics_list = [\n",
    "    PopDiversityMetric.PDC,\n",
    "    PopDiversityMetric.FDC,\n",
    "    PopDiversityMetric.PFSD,\n",
    "    PopDiversityMetric.PFM,\n",
    "]\n",
    "\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        runs = os.listdir(os.path.join(DATASET_PATH, algorithm, problem))\n",
    "        runs.sort()\n",
    "        run_path = os.path.join(DATASET_PATH, algorithm, problem, runs[0])\n",
    "        srd = SingleRunData.import_from_json(run_path)\n",
    "        pop_metrics = SingleRunData.import_from_json(run_path).get_pop_diversity_metrics_values(metrics=pop_metrics_list, minmax_scale=False, standard_scale=True)\n",
    "        ax = pop_metrics.plot(figsize=(15,7), fontsize=19, logy=False)\n",
    "        ax.set_title(label=\" \".join([f\"Populacijske metrike raznolikosti - {algorithm}\", problem]), fontdict={'fontsize':22}, pad=15)\n",
    "        ax.set_xlabel(xlabel=\"Iteracija\", fontdict={'fontsize':19}, labelpad=10)\n",
    "        ax.set_ylabel(ylabel=\"Vrednost\", fontdict={'fontsize':19}, labelpad=10)\n",
    "        ax.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_styles = ['-g', ':g', '--g', '-.g', '-b', ':b', '--b', '-.b']\n",
    "_line_styles = ['-g', '-b', '-r', '-k', ':g', ':b', ':r', ':k']\n",
    "style = {}\n",
    "pop_metrics_list = [\n",
    "    PopDiversityMetric.PDC,\n",
    "    PopDiversityMetric.FDC,\n",
    "    PopDiversityMetric.PFSD,\n",
    "    PopDiversityMetric.PFM,\n",
    "]\n",
    "style_idx = 0\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    if algorithm not in algorithms_to_plot:\n",
    "        continue\n",
    "    for idx, metric in enumerate(pop_metrics_list):\n",
    "        if idx > 3:\n",
    "            continue\n",
    "        style['_'.join([algorithm, metric.value])] = line_styles[style_idx]\n",
    "        style_idx += 1\n",
    "\n",
    "metrics_by_problem = {}\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    if algorithm not in algorithms_to_plot:\n",
    "        continue\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        runs = os.listdir(os.path.join(DATASET_PATH, algorithm, problem))\n",
    "        runs.sort()\n",
    "        run_path = os.path.join(DATASET_PATH, algorithm, problem, runs[0])\n",
    "        run = SingleRunData.import_from_json(run_path)\n",
    "        pop_metrics = run.get_pop_diversity_metrics_values(metrics=pop_metrics_list, minmax_scale=False)\n",
    "        \n",
    "        for metric in pop_metrics_list:\n",
    "            key = '_'.join([algorithm, metric.value])\n",
    "            if metric.value not in pop_metrics:\n",
    "                continue\n",
    "\n",
    "            if problem in metrics_by_problem:\n",
    "                metrics_by_problem[problem][key] = pop_metrics.get(metric.value).to_list()\n",
    "            else:\n",
    "                metric_values = {key: pop_metrics.get(metric.value).to_list()}\n",
    "                metrics_by_problem[problem] = metric_values\n",
    "\n",
    "            # scale fdc to [0, 1] for easier comparison on logy scale\n",
    "            if metric == PopDiversityMetric.FDC:\n",
    "                metrics_by_problem[problem][key] = sklearn.preprocessing.minmax_scale(\n",
    "                    metrics_by_problem[problem][key], feature_range=(0, 1)\n",
    "                )\n",
    "\n",
    "for problem in metrics_by_problem:\n",
    "    metrics = metrics_by_problem[problem]\n",
    "    df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "    ax = df_metrics.plot(style=style, figsize=(25, 7), logy=True, fontsize=15)\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_title(label=problem, fontdict={'fontsize':24})\n",
    "    ax.set_xlabel(xlabel=\"Iterations\", fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "\n",
    "    line_styles = ['-g', ':g', '--g', '-.g', '-b', ':b', '--b', '-.b']\n",
    "    _line_styles = ['-g', '-b', '-r', '-k', ':g', ':b', ':r', ':k']\n",
    "    style = {}\n",
    "    pop_metrics_list = [\n",
    "        PopDiversityMetric.PDC,\n",
    "        PopDiversityMetric.FDC,\n",
    "        PopDiversityMetric.PFSD,\n",
    "        PopDiversityMetric.PFM,\n",
    "    ]\n",
    "    style_idx = 0\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for idx, metric in enumerate(pop_metrics_list):\n",
    "            if idx > 3:\n",
    "                continue\n",
    "            style['_'.join([algorithm, metric.value])] = line_styles[style_idx]\n",
    "            style_idx += 1\n",
    "\n",
    "    metrics_by_problem = {}\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            run_path = os.path.join(dataset_path, subset, algorithm, problem, runs[0])\n",
    "            run = SingleRunData.import_from_json(run_path)\n",
    "            pop_metrics = run.get_pop_diversity_metrics_values(metrics=pop_metrics_list, minmax_scale=False)\n",
    "            \n",
    "            for metric in pop_metrics_list:\n",
    "                key = '_'.join([algorithm, metric.value])\n",
    "                if metric.value not in pop_metrics:\n",
    "                    continue\n",
    "\n",
    "                if problem in metrics_by_problem:\n",
    "                    metrics_by_problem[problem][key] = pop_metrics.get(metric.value).to_list()\n",
    "                else:\n",
    "                    metric_values = {key: pop_metrics.get(metric.value).to_list()}\n",
    "                    metrics_by_problem[problem] = metric_values\n",
    "\n",
    "                # scale fdc to [0, 1] for easier comparison on logy scale\n",
    "                if metric == PopDiversityMetric.FDC:\n",
    "                    metrics_by_problem[problem][key] = sklearn.preprocessing.minmax_scale(\n",
    "                        metrics_by_problem[problem][key], feature_range=(0, 1)\n",
    "                    )\n",
    "\n",
    "    for problem in metrics_by_problem:\n",
    "        metrics = metrics_by_problem[problem]\n",
    "        df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "        ax = df_metrics.plot(style=style, figsize=(25, 7), logy=True, fontsize=15)\n",
    "        ax.legend(fontsize=15)\n",
    "        ax.set_title(label=subset, fontdict={'fontsize':24})\n",
    "        ax.set_xlabel(xlabel=\"Iterations\", fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average population diversity metrics for all runs by subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "\n",
    "    line_styles = ['-g', ':g', '--g', '-.g', '-b', ':b', '--b', '-.b']\n",
    "    _line_styles = ['-g', '-b', '-r', '-k', ':g', ':b', ':r', ':k']\n",
    "    style = {}\n",
    "    pop_metrics_list = [\n",
    "        PopDiversityMetric.PDC,\n",
    "        PopDiversityMetric.FDC,\n",
    "        PopDiversityMetric.PFSD,\n",
    "        PopDiversityMetric.PFM,\n",
    "    ]\n",
    "    style_idx = 0\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for idx, metric in enumerate(pop_metrics_list):\n",
    "            if idx > 3:\n",
    "                continue\n",
    "            style['_'.join([algorithm, metric.value])] = line_styles[style_idx]\n",
    "            style_idx += 1\n",
    "\n",
    "    metrics_by_problem = {}\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            for run_file_name in runs:\n",
    "                run_path = os.path.join(dataset_path, subset, algorithm, problem, run_file_name)\n",
    "                run = SingleRunData.import_from_json(run_path)\n",
    "                pop_metrics = run.get_pop_diversity_metrics_values(metrics=pop_metrics_list, minmax_scale=False)\n",
    "                \n",
    "                for metric in pop_metrics_list:\n",
    "                    key = '_'.join([algorithm, metric.value])\n",
    "                    if metric.value not in pop_metrics:\n",
    "                        continue\n",
    "\n",
    "                    if problem in metrics_by_problem:\n",
    "                        if key in metrics_by_problem[problem]:\n",
    "                            sum_of_metrics = np.add(\n",
    "                                metrics_by_problem[problem][key], pop_metrics.get(metric.value).to_numpy() / len(runs)\n",
    "                            )\n",
    "                            metrics_by_problem[problem][key] = sum_of_metrics\n",
    "                        else:\n",
    "                            metrics_by_problem[problem][key] = pop_metrics.get(metric.value).to_numpy() / len(runs)\n",
    "                    else:\n",
    "                        metric_values = {key: pop_metrics.get(metric.value).to_numpy() / len(runs)}\n",
    "                        metrics_by_problem[problem] = metric_values\n",
    "\n",
    "                    # scale fdc to [0, 1] for easier comparison on logy scale\n",
    "                    \"\"\"\n",
    "                    if metric == PopDiversityMetric.FDC:\n",
    "                        metrics_by_problem[problem][key] = sklearn.preprocessing.minmax_scale(\n",
    "                            metrics_by_problem[problem][key], feature_range=(0, 1)\n",
    "                        )\n",
    "                    \"\"\"\n",
    "\n",
    "    for problem in metrics_by_problem:\n",
    "        metrics = metrics_by_problem[problem]\n",
    "        df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "        ax = df_metrics.plot(style=style, figsize=(25, 7), logy=True, fontsize=15)\n",
    "        ax.legend(fontsize=15)\n",
    "        ax.set_title(label=subset, fontdict={'fontsize':24})\n",
    "        ax.set_xlabel(xlabel=\"Iterations\", fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual diversity metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_problem = {}\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    if algorithm not in [\"WVCPSO\"]:#algorithms_to_plot:\n",
    "        continue\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        runs = os.listdir(os.path.join(DATASET_PATH, algorithm, problem))\n",
    "        runs.sort()\n",
    "        run_path = os.path.join(DATASET_PATH, algorithm, problem, runs[0])\n",
    "        run = SingleRunData.import_from_json(run_path)\n",
    "        indiv_metrics = run.get_indiv_diversity_metrics_values(minmax_scale=False, standard_scale=True)\n",
    "        for metric in INDIV_DIVERSITY_METRICS:\n",
    "            key = '_'.join([algorithm, metric.value])\n",
    "            if problem in metrics_by_problem:\n",
    "                metrics_by_problem[problem][key] = indiv_metrics.get(metric.value).to_list()\n",
    "            else:\n",
    "                metric_values = {key: indiv_metrics.get(metric.value).to_list()}\n",
    "                metrics_by_problem[problem] = metric_values\n",
    "        \n",
    "\n",
    "for problem in metrics_by_problem:\n",
    "    metrics = metrics_by_problem[problem]\n",
    "    df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(INDIV_DIVERSITY_METRICS))\n",
    "    fig.subplots_adjust(wspace=0.7, top=0.825, bottom=0)\n",
    "    #fig.suptitle(problem, fontsize=20)\n",
    "    fig.suptitle(\"Individualne metrike raznolikosti - Schwefel\", fontsize=22)\n",
    "\n",
    "    for idx, metric in enumerate(INDIV_DIVERSITY_METRICS):\n",
    "        df_metric = df_metrics.filter(regex=metric.value)\n",
    "        df_metric.columns = df_metric.columns.str.replace(\"WVCPSO\" + '_'+metric.value, 'PSO')\n",
    "        ax = df_metric.plot(ax=axes[idx], kind=\"box\", figsize=(15, 5), logy=False, fontsize=19)\n",
    "        ax.margins(x=0)\n",
    "        ax.set_title(label=metric.name, fontdict={'fontsize':19}, pad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "\n",
    "    metrics_by_problem = {}\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            run_path = os.path.join(dataset_path, subset, algorithm, problem, runs[0])\n",
    "            run = SingleRunData.import_from_json(run_path)\n",
    "            indiv_metrics = run.get_indiv_diversity_metrics_values(minmax_scale=False, standard_scale=True)\n",
    "            for metric in INDIV_DIVERSITY_METRICS:\n",
    "                key = '_'.join([algorithm, metric.value])\n",
    "                if problem in metrics_by_problem:\n",
    "                    metrics_by_problem[problem][key] = indiv_metrics.get(metric.value).to_list()\n",
    "                else:\n",
    "                    metric_values = {key: indiv_metrics.get(metric.value).to_list()}\n",
    "                    metrics_by_problem[problem] = metric_values\n",
    "            \n",
    "\n",
    "    for problem in metrics_by_problem:\n",
    "        metrics = metrics_by_problem[problem]\n",
    "        df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(INDIV_DIVERSITY_METRICS))\n",
    "        fig.suptitle(f\"{problem} {subset}\", fontsize=23)\n",
    "        fig.subplots_adjust(wspace=0.4)\n",
    "\n",
    "        for idx, metric in enumerate(INDIV_DIVERSITY_METRICS):\n",
    "            df_metric = df_metrics.filter(regex=metric.value)\n",
    "            df_metric.columns = df_metric.columns.str.replace('_'+metric.value, '')\n",
    "            ax = df_metric.plot(ax=axes[idx], kind=\"box\", figsize=(25, 6), logy=False, fontsize=15)\n",
    "            ax.set_title(label=metric.name, fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average of individual diversity metrics for all runs by subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "\n",
    "    metrics_by_problem = {}\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            for run_file_name in runs:\n",
    "                run_path = os.path.join(dataset_path, subset, algorithm, problem, runs[0])\n",
    "                run = SingleRunData.import_from_json(run_path)\n",
    "                indiv_metrics = run.get_indiv_diversity_metrics_values(minmax_scale=False)\n",
    "                for metric in INDIV_DIVERSITY_METRICS:\n",
    "                    key = '_'.join([algorithm, metric.value])\n",
    "                    if problem in metrics_by_problem:\n",
    "                        if key in metrics_by_problem[problem]:\n",
    "                            sum_of_metrics = np.add(\n",
    "                                metrics_by_problem[problem][key], indiv_metrics.get(metric.value).to_numpy() / len(runs)\n",
    "                            )\n",
    "                            metrics_by_problem[problem][key] = sum_of_metrics\n",
    "                        else:    \n",
    "                            metrics_by_problem[problem][key] = indiv_metrics.get(metric.value).to_numpy() / len(runs)\n",
    "                    else:\n",
    "                        metric_values = {key: indiv_metrics.get(metric.value).to_numpy() / len(runs)}\n",
    "                        metrics_by_problem[problem] = metric_values\n",
    "            \n",
    "\n",
    "    for problem in metrics_by_problem:\n",
    "        metrics = metrics_by_problem[problem]\n",
    "        df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(INDIV_DIVERSITY_METRICS))\n",
    "        fig.suptitle(f\"{problem} {subset}\", fontsize=23)\n",
    "        fig.subplots_adjust(wspace=0.4)\n",
    "\n",
    "        for idx, metric in enumerate(INDIV_DIVERSITY_METRICS):\n",
    "            df_metric = df_metrics.filter(regex=metric.value)\n",
    "            df_metric.columns = df_metric.columns.str.replace('_'+metric.value, '')\n",
    "            ax = df_metric.plot(ax=axes[idx], kind=\"box\", figsize=(25, 6), logy=True, fontsize=15)\n",
    "            ax.set_title(label=metric.name, fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-SMAPE & feature vector comparison (normalized and pairwise normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "    #------------------------------------------------------------------------------------\n",
    "    #   Diversity metrics 1-SMAPE\n",
    "    #------------------------------------------------------------------------------------\n",
    "    al1 = \"WVCPSO\"\n",
    "    al2 = \"FA\"\n",
    "    problem = \"Schwefel\"\n",
    "\n",
    "\n",
    "    first_runs = os.listdir(os.path.join(dataset_path, subset, al1, problem))\n",
    "    second_runs = os.listdir(os.path.join(dataset_path, subset, al2, problem))\n",
    "\n",
    "    first_runs.sort()\n",
    "    second_runs.sort()\n",
    "\n",
    "    mean_similarities = []\n",
    "    max_similarities = []\n",
    "    mean_similarity_values = []\n",
    "    max_similarity_values = []\n",
    "\n",
    "    for fr, sr in zip(first_runs, second_runs):\n",
    "        first_run_path = os.path.join(dataset_path, subset, al1, problem, fr)\n",
    "        second_run_path = os.path.join(dataset_path, subset, al2, problem, sr)\n",
    "        f_srd = SingleRunData.import_from_json(first_run_path)\n",
    "        s_srd = SingleRunData.import_from_json(second_run_path)\n",
    "        \n",
    "        similarity = f_srd.get_diversity_metrics_similarity(s_srd, get_raw_values=True)\n",
    "        mean_similarity_values.append(np.mean(similarity))\n",
    "        mean_similarities.append(np.mean(mean_similarity_values))\n",
    "        max_similarity_values.append(np.max(similarity))\n",
    "        max_similarities.append(np.mean(max_similarity_values))\n",
    "\n",
    "    fig, ax, = plt.subplots(1, 3, figsize=(25, 5))\n",
    "    ax[0].plot(np.arange(0, len(mean_similarities)), mean_similarities)\n",
    "\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "    #   Pairwise feature vector comparison\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    feature_vectors_1 = []\n",
    "    feature_vectors_2 = []\n",
    "    for idx, algorithm in enumerate(os.listdir(os.path.join(dataset_path, subset))):\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            for run in runs:\n",
    "                run_path = os.path.join(dataset_path, subset, algorithm, problem, run)\n",
    "                srd = SingleRunData.import_from_json(run_path)\n",
    "            \n",
    "                feature_vector = srd.get_feature_vector()\n",
    "\n",
    "                if idx == 0:\n",
    "                    feature_vectors_1.append(feature_vector)\n",
    "                else:\n",
    "                    feature_vectors_2.append(feature_vector)\n",
    "\n",
    "    # similarity plot for variable number of vectors\n",
    "    # cosine\n",
    "    mean_similarities = []\n",
    "    for i in range(2, len(feature_vectors_1)):\n",
    "        mean_vector_1 = np.mean(feature_vectors_1[:i], axis=0)\n",
    "        mean_vector_2 = np.mean(feature_vectors_2[:i], axis=0)\n",
    "        mean_similarities.append(1 - spatial.distance.cosine(mean_vector_1, mean_vector_2))\n",
    "\n",
    "    mean_pairwise_similarities = []\n",
    "    for i in range(2, len(feature_vectors_1)):\n",
    "        similarities = []\n",
    "        for feature_vector1, feature_vector2 in zip(feature_vectors_1[:i], feature_vectors_2[:i]):\n",
    "            similarities.append(1 - spatial.distance.cosine(feature_vector1, feature_vector2))\n",
    "\n",
    "        mean_pairwise_similarities.append(np.mean(similarities))\n",
    "\n",
    "    # Spearman\n",
    "    mean_spearman = []\n",
    "    for i in range(2, len(feature_vectors_1)):\n",
    "        mean_vector_1 = np.mean(feature_vectors_1[:i], axis=0)\n",
    "        mean_vector_2 = np.mean(feature_vectors_2[:i], axis=0)\n",
    "        r, p = stats.spearmanr(mean_vector_1, mean_vector_2)\n",
    "        mean_spearman.append(r)\n",
    "\n",
    "    mean_pairwise_spearman = []\n",
    "    for i in range(2, len(feature_vectors_1)):\n",
    "        spearman_values = []\n",
    "        for feature_vector1, feature_vector2 in zip(feature_vectors_1[:i], feature_vectors_2[:i]):\n",
    "            r, p = stats.spearmanr(feature_vector1, feature_vector2)\n",
    "            spearman_values.append(r)\n",
    "\n",
    "        mean_pairwise_spearman.append(np.mean(spearman_values))\n",
    "\n",
    "    ax[1].plot(np.arange(0, len(mean_pairwise_similarities)), mean_pairwise_similarities, label=\"pairwise cosine\")\n",
    "    ax[1].plot(np.arange(0, len(mean_similarities)), mean_similarities, label=\"mean cosine\")\n",
    "    ax[2].plot(np.arange(0, len(mean_pairwise_spearman)), mean_pairwise_spearman, label=\"pairwise Spearman\")\n",
    "    ax[2].plot(np.arange(0, len(mean_spearman)), mean_spearman, label=\"mean Spearman\")\n",
    "    ax[1].legend()\n",
    "    ax[2].legend()\n",
    "\n",
    "    fig.suptitle(subset)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-SMAPE values plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "mean_similarity_values = []\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "    al1 = \"WVCPSO\"\n",
    "    al2 = \"FA\"\n",
    "    problem = \"Schwefel\"\n",
    "\n",
    "    first_runs = os.listdir(os.path.join(dataset_path, subset, al1, problem))\n",
    "    second_runs = os.listdir(os.path.join(dataset_path, subset, al2, problem))\n",
    "\n",
    "    first_runs.sort()\n",
    "    second_runs.sort()\n",
    "\n",
    "    sum = 0.0\n",
    "    similarity_values = []\n",
    "    mean_similarity = []\n",
    "    for fr, sr in zip(first_runs, second_runs):\n",
    "        first_run_path = os.path.join(dataset_path, subset, al1, problem, fr)\n",
    "        second_run_path = os.path.join(dataset_path, subset, al2, problem, sr)\n",
    "        f_srd = SingleRunData.import_from_json(first_run_path)\n",
    "        s_srd = SingleRunData.import_from_json(second_run_path)\n",
    "        \n",
    "        similarity = f_srd.get_diversity_metrics_similarity(s_srd, get_raw_values=True)\n",
    "        similarity_values.append(similarity)\n",
    "\n",
    "    # x axis labels\n",
    "    labels = []\n",
    "    for metric in POP_DIVERSITY_METRICS:\n",
    "        labels.append(metric.value)\n",
    "    for metric in INDIV_DIVERSITY_METRICS:\n",
    "        labels.append(metric.value)\n",
    "\n",
    "    similarity_values = np.array(similarity_values)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.violinplot(similarity_values, showmeans=True, showextrema=True, showmedians=True)\n",
    "    plt.xticks(ticks=np.arange(1, len(labels) + 1), labels=labels, fontsize=19)\n",
    "    plt.yticks(fontsize=19)\n",
    "    plt.ylabel(\"vrednost\", fontsize=19, labelpad=10)\n",
    "    plt.xlabel(\"metrika\", fontsize=19, labelpad=10)\n",
    "    plt.title(f\"primerjava algoritmov PSO in FA po metriki 1-SMAPE subset {subset}\", fontsize=22, pad=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature vectors comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al1 = \"WVCPSO\"\n",
    "al2 = \"FA\"\n",
    "problem = \"Schwefel\"\n",
    "\n",
    "feature_vectors_1 = []\n",
    "feature_vectors_2 = []\n",
    "\n",
    "first_runs = os.listdir(os.path.join(DATASET_PATH, al1, problem))\n",
    "second_runs = os.listdir(os.path.join(DATASET_PATH, al2, problem))\n",
    "\n",
    "first_runs.sort()\n",
    "second_runs.sort()\n",
    "\n",
    "first_srd_list = []\n",
    "second_srd_list = []\n",
    "\n",
    "sum = 0.0\n",
    "similarities = []\n",
    "for fr, sr in zip(first_runs, second_runs):\n",
    "    first_run_path = os.path.join(DATASET_PATH, al1, problem, fr)\n",
    "    second_run_path = os.path.join(DATASET_PATH, al2, problem, sr)\n",
    "    f_srd = SingleRunData.import_from_json(first_run_path)\n",
    "    s_srd = SingleRunData.import_from_json(second_run_path)\n",
    "    fv1 = f_srd.get_feature_vector(standard_scale=True, minmax_scale=False)\n",
    "    fv2 = s_srd.get_feature_vector(standard_scale=True, minmax_scale=False)\n",
    "\n",
    "    feature_vectors_1.append(fv1)\n",
    "    feature_vectors_2.append(fv2)\n",
    "    #first_srd_list.append(f_srd)\n",
    "    #second_srd_list.append(s_srd)\n",
    "\n",
    "#feature_vectors_1, feature_vectors_2 = SingleRunData.get_feature_vectors_global_diversity_metrics_scaling(first_srd_list=first_srd_list, second_srd_list=second_srd_list)\n",
    "\n",
    "# similarity plot for variable number of vectors\n",
    "mean_similarities = []\n",
    "for i in range(2, len(feature_vectors_1)):\n",
    "    mean_vector_1 = np.mean(feature_vectors_1[:i], axis=0)\n",
    "    mean_vector_2 = np.mean(feature_vectors_2[:i], axis=0)\n",
    "    mean_similarities.append(1 - spatial.distance.cosine(mean_vector_1, mean_vector_2))\n",
    "\n",
    "mean_pairwise_similarities = []\n",
    "for i in range(2, len(feature_vectors_1)):\n",
    "    similarities = []\n",
    "    euclidean_distance = []\n",
    "    for feature_vector1, feature_vector2 in zip(feature_vectors_1[:i], feature_vectors_2[:i]):\n",
    "        similarities.append(1 - spatial.distance.cosine(feature_vector1, feature_vector2))\n",
    "\n",
    "    mean_pairwise_similarities.append(np.mean(similarities))\n",
    "\n",
    "plt.plot(np.arange(0, len(mean_pairwise_similarities)), mean_pairwise_similarities)\n",
    "plt.plot(np.arange(0, len(mean_similarities)), mean_similarities)\n",
    "plt.show()\n",
    "\n",
    "# plot mean vectors for visual comparison\n",
    "mean_vector_1 = np.mean(feature_vectors_1, axis=0)\n",
    "mean_vector_2 = np.mean(feature_vectors_2, axis=0)\n",
    "\n",
    "r, p = stats.spearmanr(mean_vector_1, mean_vector_2)\n",
    "print(\"cosine similarity of average vectors: \", 1 - spatial.distance.cosine(mean_vector_1, mean_vector_2), \"\\n\")\n",
    "print(f\"person test for average vectors: r = {r}, p = {p}\")\n",
    "\n",
    "index = np.arange(len(mean_vector_1))\n",
    "bar_width = 0.4\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.gca()\n",
    "plt.bar(index, mean_vector_1, bar_width, label=\"PSO\")\n",
    "plt.bar(index + bar_width, mean_vector_2, bar_width, label=al2)\n",
    "plt.xlabel(\"značilnica\", fontsize=19, labelpad=15)\n",
    "plt.ylabel(\"vrednost\", fontsize=19)\n",
    "plt.xticks(index + bar_width / 2, index, fontsize=19, rotation=45)\n",
    "plt.yticks(fontsize=19)\n",
    "plt.xlim(ax.patches[0].get_x() * 2.25, ax.patches[-1].get_x() + ax.patches[-1].get_width() * 1.5)\n",
    "plt.grid(axis = \"y\", color = 'gray', linestyle = '--', linewidth = 0.5)\n",
    "ax.set_axisbelow(True)\n",
    "plt.title(f\"Primerjava povprečnih vektorjev značilnic PSO - FA\", fontsize=22, pad=15)\n",
    "plt.show()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mean_vector_1 = scaler.fit_transform(mean_vector_1.reshape((-1,1))).reshape((-1))\n",
    "mean_vector_2 = scaler.transform(mean_vector_2.reshape((-1,1))).reshape((-1))\n",
    "\n",
    "index = np.arange(len(mean_vector_1))\n",
    "bar_width = 0.4\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.gca()\n",
    "plt.bar(index, mean_vector_1, bar_width, label=\"PSO\")\n",
    "plt.bar(index + bar_width, mean_vector_2, bar_width, label=al2)\n",
    "plt.xlabel(\"značilnica\", fontsize=19, labelpad=15)\n",
    "plt.ylabel(\"vrednost\", fontsize=19)\n",
    "plt.xticks(index + bar_width / 2, index, fontsize=19, rotation=45)\n",
    "plt.yticks(fontsize=19)\n",
    "plt.xlim(ax.patches[0].get_x() * 2.25, ax.patches[-1].get_x() + ax.patches[-1].get_width() * 1.5)\n",
    "plt.grid(axis = \"y\", color = 'gray', linestyle = '--', linewidth = 0.5)\n",
    "ax.set_axisbelow(True)\n",
    "plt.title(f\"Primerjava povprečnih vektorjev značilnic PSO - FA\", fontsize=22, pad=15)\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "similarities = []\n",
    "for feature_vector1, feature_vector2 in zip(feature_vectors_1, feature_vectors_2):\n",
    "    #print(1 - spatial.distance.cosine(feature_vector1, feature_vector2))\n",
    "    similarities.append(1 - spatial.distance.cosine(feature_vector1, feature_vector2))\n",
    "\n",
    "print(\"pairwise similarity\",np.mean(similarities))\n",
    "\n",
    "for vector_idx in range(4):\n",
    "    plt.plot(np.arange(0, len(feature_vectors_1[vector_idx])), feature_vectors_1[vector_idx])\n",
    "    plt.plot(np.arange(0, len(feature_vectors_2[vector_idx])), feature_vectors_2[vector_idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FDC comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdc_values = {}\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    if algorithm not in algorithms_to_plot:\n",
    "        continue\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        runs = os.listdir(os.path.join(DATASET_PATH, algorithm, problem))\n",
    "        runs.sort()\n",
    "        run_path = os.path.join(DATASET_PATH, algorithm, problem, runs[0])\n",
    "        run = SingleRunData.import_from_json(run_path)\n",
    "        metrics = run.get_pop_diversity_metrics_values(metrics=[PopDiversityMetric.FDC], minmax_scale=False)\n",
    "        if len(metrics) == 0:\n",
    "            continue\n",
    "        fdc = metrics.get(\"FDC\")\n",
    "        \n",
    "        if problem in fdc_values:\n",
    "            fdc_values[problem][algorithm] = fdc\n",
    "        else:\n",
    "            fdc_dict = {algorithm: fdc}\n",
    "            fdc_values[problem] = fdc_dict\n",
    "\n",
    "for problem in fdc_values:\n",
    "    fdc_dict = fdc_values[problem]\n",
    "    for key in fdc_dict:\n",
    "        convergence = fdc_dict[key]\n",
    "        fdc_dict[key] = convergence\n",
    "\n",
    "    fdc_dict = pd.DataFrame.from_dict(fdc_dict)\n",
    "    ax = fdc_dict.plot(title=problem, figsize=(25, 7), logy=False, fontsize=15)\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_title(label=problem, fontdict={'fontsize':24})\n",
    "    ax.set_xlabel(xlabel=\"Iterations\", fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best fitness value convergence comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergences = {}\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    if algorithm not in algorithms_to_plot:\n",
    "        continue\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        runs = os.listdir(os.path.join(DATASET_PATH, algorithm, problem))\n",
    "        runs.sort()\n",
    "        run_path = os.path.join(DATASET_PATH, algorithm, problem, runs[0])\n",
    "        run = SingleRunData.import_from_json(run_path)\n",
    "        print(f\"best fitness {algorithm} - {problem}: {run.best_fitness}\")\n",
    "        convergence = run.get_best_fitness_values(normalize=False)\n",
    "        \n",
    "        if problem in convergences:\n",
    "            convergences[problem][algorithm] = convergence\n",
    "        else:\n",
    "            convergence_dict = {algorithm: convergence}\n",
    "            convergences[problem] = convergence_dict\n",
    "\n",
    "for problem in convergences:\n",
    "    convergence_dict = convergences[problem]\n",
    "    for key in convergence_dict:\n",
    "        convergence = convergence_dict[key]\n",
    "        convergence_dict[key] = convergence\n",
    "\n",
    "    convergence_dict = pd.DataFrame.from_dict(convergence_dict)\n",
    "    ax = convergence_dict.plot(title=problem, figsize=(25, 7), logy=False, fontsize=15)\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_title(label=problem, fontdict={'fontsize':24})\n",
    "    ax.set_xlabel(xlabel=\"Iterations\", fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN training and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader, val_data_loader, test_data_loader, actual_labels = get_data_loaders(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    val_size=VAL_SIZE,\n",
    "    test_size=TEST_SIZE,\n",
    "    n_pca_components=N_PCA_COMPONENTS,\n",
    "    problems=[OPTIMIZATION_PROBLEM.name()],\n",
    "    dataset_subsets=False,\n",
    "    random_state=RNG_SEED\n",
    ")\n",
    "\n",
    "pop_features, indiv_features, target = next(iter(train_data_loader))\n",
    "lstm_model = LSTMClassifier(\n",
    "    input_dim=np.shape(pop_features)[2],\n",
    "    aux_input_dim=np.shape(indiv_features)[1],\n",
    "    num_labels=len(actual_labels),\n",
    "    hidden_dim=LSTM_HIDDEN_DIM,\n",
    "    num_layers=LSTM_NUM_LAYERS,\n",
    "    dropout=LSTM_DROPOUT\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lstm_model_filename = f\"./lstm_model.pt\"\n",
    "\n",
    "if execute_training:\n",
    "    lstm_model.to(device)\n",
    "    nn_train(\n",
    "        model=lstm_model,\n",
    "        train_data_loader=train_data_loader,\n",
    "        val_data_loader=val_data_loader,\n",
    "        epochs=EPOCHS,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        model_filename=lstm_model_filename,\n",
    "        verbal=True)\n",
    "else:\n",
    "    lstm_model = torch.load(lstm_model_filename, map_location=torch.device(device))\n",
    "    lstm_model.to(device)\n",
    "    if os.path.exists('loss_plot.png'):\n",
    "        loss_plot = np.asarray(Image.open('loss_plot.png'))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test(\n",
    "    model=lstm_model,\n",
    "    test_data_loader=test_data_loader,\n",
    "    device=device,\n",
    "    labels=actual_labels,\n",
    "    show_classification_report=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader, val_data_loader, test_data_loader, actual_labels = get_data_loaders(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    batch_size=100,\n",
    "    val_size=VAL_SIZE,\n",
    "    test_size=TEST_SIZE,\n",
    "    n_pca_components=N_PCA_COMPONENTS,\n",
    "    problems=[OPTIMIZATION_PROBLEM.name()],\n",
    "    dataset_subsets=True,\n",
    "    nn_type=NNType.LINEAR,\n",
    "    random_state=RNG_SEED\n",
    ")\n",
    "\n",
    "feature_vector, target = next(iter(train_data_loader))\n",
    "\n",
    "linear_model = LinearClassifier(\n",
    "    input_dim=np.shape(feature_vector)[1],\n",
    "    num_labels=len(actual_labels),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(linear_model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "linear_model_filename = f\"./linear_model.pt\"\n",
    "\n",
    "if execute_training:\n",
    "    linear_model.to(device)\n",
    "    nn_train(\n",
    "        model=linear_model,\n",
    "        train_data_loader=train_data_loader,\n",
    "        val_data_loader=val_data_loader,\n",
    "        epochs=200,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        model_filename=linear_model_filename,\n",
    "        nn_type=NNType.LINEAR,\n",
    "        verbal=True)\n",
    "else:\n",
    "    linear_model = torch.load(linear_model_filename, map_location=torch.device(device))\n",
    "    linear_model.to(device)\n",
    "    if os.path.exists('loss_plot.png'):\n",
    "        loss_plot = np.asarray(Image.open('loss_plot.png'))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(loss_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test(\n",
    "    model=linear_model,\n",
    "    test_data_loader=test_data_loader,\n",
    "    device=device,\n",
    "    nn_type=NNType.LINEAR,\n",
    "    labels=actual_labels,\n",
    "    show_classification_report=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity metrics calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('standardized_ml/fa_pso_random.json', 'r') as file:\n",
    "    ml_accuracy = json.load(file)\n",
    "\n",
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "mean_similarity_values = []\n",
    "mean_cosine_similarity_values = []\n",
    "mean_spearman_r = []\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "    al1 = \"FA\"\n",
    "    al2 = \"WVCPSO\"\n",
    "    problem = \"Schwefel\"\n",
    "\n",
    "    first_runs = os.listdir(os.path.join(dataset_path, subset, al1, problem))\n",
    "    second_runs = os.listdir(os.path.join(dataset_path, subset, al2, problem))\n",
    "\n",
    "    first_runs.sort()\n",
    "    second_runs.sort()\n",
    "    # *******************************************************************************\n",
    "    # 1-SMAPE\n",
    "    # *******************************************************************************\n",
    "    similarity_values = []\n",
    "    mean_similarity = []\n",
    "    for fr, sr in zip(first_runs, second_runs):\n",
    "        first_run_path = os.path.join(dataset_path, subset, al1, problem, fr)\n",
    "        second_run_path = os.path.join(dataset_path, subset, al2, problem, sr)\n",
    "        f_srd = SingleRunData.import_from_json(first_run_path)\n",
    "        s_srd = SingleRunData.import_from_json(second_run_path)\n",
    "        \n",
    "        similarity = f_srd.get_diversity_metrics_similarity(s_srd, get_raw_values=True)\n",
    "        mean_similarity.append(round(np.mean(similarity), 2))\n",
    "        similarity_values.append(similarity)\n",
    "\n",
    "    mean_similarity_values.append(round(np.mean(mean_similarity), 2))\n",
    "\n",
    "    # *******************************************************************************\n",
    "    # Cosine and Spearman\n",
    "    # *******************************************************************************\n",
    "\n",
    "    feature_vectors_1 = []\n",
    "    feature_vectors_2 = []\n",
    "    for idx, algorithm in enumerate(os.listdir(os.path.join(dataset_path, subset))):\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            for run in runs:\n",
    "                run_path = os.path.join(dataset_path, subset, algorithm, problem, run)\n",
    "                srd = SingleRunData.import_from_json(run_path)\n",
    "            \n",
    "                feature_vector = srd.get_feature_vector(standard_scale=True)\n",
    "\n",
    "                if idx == 0:\n",
    "                    feature_vectors_1.append(feature_vector)\n",
    "                else:\n",
    "                    feature_vectors_2.append(feature_vector)\n",
    "\n",
    "    mean_vector_1 = np.mean(feature_vectors_1, axis=0)\n",
    "    mean_vector_2 = np.mean(feature_vectors_2, axis=0)\n",
    "\n",
    "    mean_cosine_similarity_values.append(1 - spatial.distance.cosine(mean_vector_1, mean_vector_2))\n",
    "    r, p = stats.spearmanr(mean_vector_1, mean_vector_2)\n",
    "    mean_spearman_r.append(r)\n",
    "\n",
    "for similarity, cosine, r, svm_test, knn_test in zip(mean_similarity_values, mean_cosine_similarity_values, mean_spearman_r, ml_accuracy[\"svm_test\"], ml_accuracy[\"knn_test\"]):\n",
    "    print(f\"{round(similarity, 2)} & {round(cosine, 2)} & {round(r, 2)} & {round(svm_test, 2)} & {round(knn_test, 2)}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for similarity, cosine, r, svm_test, knn_test in zip(mean_similarity_values, mean_cosine_similarity_values, mean_spearman_r, ml_accuracy[\"svm_test\"], ml_accuracy[\"knn_test\"]):\n",
    "    output = \"\"\n",
    "    if round(similarity, 2) == round(np.max(mean_similarity_values), 2):\n",
    "        output += \"\\\\textbf{\" + f\"{round(similarity, 2)}\" + \"} & \"\n",
    "    else:\n",
    "        output += f\"{round(similarity, 2)} & \"\n",
    "\n",
    "    if round(cosine, 2) == round(np.max(mean_cosine_similarity_values), 2):\n",
    "        output += \"\\\\textbf{\" + f\"{round(cosine, 2)}\" + \"} & \"\n",
    "    else:\n",
    "        output += f\"{round(cosine, 2)} & \"\n",
    "\n",
    "    if round(r, 2) == round(np.max(mean_spearman_r), 2):\n",
    "        output += \"\\\\textbf{\" + f\"{round(r, 2)}\" + \"} & \"\n",
    "    else:\n",
    "        output += f\"{round(r, 2)} & \"\n",
    "\n",
    "    if round(svm_test, 2) == round(np.max(ml_accuracy[\"svm_test\"]), 2):\n",
    "        output += \"\\\\textbf{\" + f\"{round(svm_test, 2)}\" + \"} & \"\n",
    "    else:\n",
    "        output += f\"{round(svm_test, 2)} & \"       \n",
    "\n",
    "    if round(knn_test, 2) == round(np.max(ml_accuracy[\"knn_test\"]), 2):\n",
    "        output += \"\\\\textbf{\" + f\"{round(knn_test, 2)}\" + \"}\"\n",
    "    else:\n",
    "        output += f\"{round(knn_test, 2)}\"\n",
    "    print(output)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(f\" & {round(np.min(mean_similarity_values), 2)} & {round(np.min(mean_cosine_similarity_values), 2)} & {round(np.min(mean_spearman_r), 2)} & {round(np.min(ml_accuracy['svm_test']), 2)}  & {round(np.min(ml_accuracy['knn_test']), 2)} \")\n",
    "print(f\" & {round(np.mean(mean_similarity_values), 2)} & {round(np.mean(mean_cosine_similarity_values), 2)} & {round(np.mean(mean_spearman_r), 2)} & {round(np.mean(ml_accuracy['svm_test']), 2)} & {round(np.mean(ml_accuracy['knn_test']), 2)}\")\n",
    "print(f\" & {round(np.max(mean_similarity_values), 2)} & {round(np.max(mean_cosine_similarity_values), 2)} & {round(np.max(mean_spearman_r), 2)} & {round(np.max(ml_accuracy['svm_test']), 2)}  & {round(np.max(ml_accuracy['knn_test']), 2)} \")\n",
    "print(f\" & {round(np.std(mean_similarity_values), 2)} & {round(np.std(mean_cosine_similarity_values), 2)} & {round(np.std(mean_spearman_r), 2)} & {round(np.std(ml_accuracy['svm_test']), 2)}  & {round(np.std(ml_accuracy['knn_test']), 2)} \")\n",
    "\n",
    "print(f\"{round(np.min(mean_similarity_values), 2)} \")\n",
    "print(f\"{round(np.mean(mean_similarity_values), 2)}\")\n",
    "print(f\"{round(np.max(mean_similarity_values), 2)} \")\n",
    "print(f\"{round(np.std(mean_similarity_values), 2)} \")\n",
    "\n",
    "print(f\"{round(np.min(mean_cosine_similarity_values), 2)} \")\n",
    "print(f\"{round(np.mean(mean_cosine_similarity_values), 2)}\")\n",
    "print(f\"{round(np.max(mean_cosine_similarity_values), 2)} \")\n",
    "print(f\"{round(np.std(mean_cosine_similarity_values), 2)} \")\n",
    "\n",
    "print(f\"{round(np.min(mean_spearman_r), 2)} \")\n",
    "print(f\"{round(np.mean(mean_spearman_r), 2)}\")\n",
    "print(f\"{round(np.max(mean_spearman_r), 2)} \")\n",
    "print(f\"{round(np.std(mean_spearman_r), 2)} \")\n",
    "\n",
    "print(f\"{round(np.min(ml_accuracy['svm_test']), 2)} \")\n",
    "print(f\"{round(np.mean(ml_accuracy['svm_test']), 2)}\")\n",
    "print(f\"{round(np.max(ml_accuracy['svm_test']), 2)} \")\n",
    "print(f\"{round(np.std(ml_accuracy['svm_test']), 2)} \")\n",
    "\n",
    "print(f\"{round(np.min(ml_accuracy['knn_test']), 2)} \")\n",
    "print(f\"{round(np.mean(ml_accuracy['knn_test']), 2)}\")\n",
    "print(f\"{round(np.max(ml_accuracy['knn_test']), 2)} \")\n",
    "print(f\"{round(np.std(ml_accuracy['knn_test']), 2)} \")\n",
    "\n",
    "index = np.arange(len(mean_similarity_values))\n",
    "bar_width = 0.2\n",
    "fig = plt.figure(figsize=(25, 7))\n",
    "ax = plt.gca()\n",
    "plt.bar(index, mean_similarity_values, width=bar_width, label=\"1-SMAPE\")\n",
    "plt.bar(index+bar_width, mean_cosine_similarity_values, width=bar_width, label=\"cosim\")\n",
    "plt.bar(index+(bar_width*2), mean_spearman_r, width=bar_width, label=\"rho\")\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.xticks(index + bar_width*2 / 2, index, fontsize=19, rotation=45)\n",
    "plt.xlim(ax.patches[0].get_x() * 2.25, ax.patches[-1].get_x() + ax.patches[-1].get_width() * 1.5)\n",
    "plt.yticks(fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_idx = np.argsort(mean_similarity_values)\n",
    "tmp_mean_similarity_values = np.array(mean_similarity_values)[sorting_idx]\n",
    "tmp_mean_cosine_similarity_values = np.array(mean_cosine_similarity_values)[sorting_idx]\n",
    "tmp_mean_spearman_r = np.array(mean_spearman_r)[sorting_idx]\n",
    "\n",
    "index = np.arange(len(mean_similarity_values))\n",
    "bar_width = 0.2\n",
    "fig = plt.figure(figsize=(25, 7))\n",
    "ax = plt.gca()\n",
    "plt.bar(index, tmp_mean_similarity_values, width=bar_width, label=\"1-SMAPE\")\n",
    "plt.bar(index+bar_width, tmp_mean_cosine_similarity_values, width=bar_width, label=\"cosim\")\n",
    "plt.bar(index+(bar_width*2), tmp_mean_spearman_r, width=bar_width, label=\"rho\")\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.xticks(index + bar_width*2 / 2, index, fontsize=19, rotation=45)\n",
    "plt.xlim(ax.patches[0].get_x() * 2.25, ax.patches[-1].get_x() + ax.patches[-1].get_width() * 1.5)\n",
    "plt.yticks(fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity metrics correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data_labels = [\"1-SMAPE\", \"cosim\", \"rho\", \"SVM test\", \"KNN test\"]\n",
    "\n",
    "corr_data = np.array([\n",
    "    mean_similarity_values, \n",
    "    mean_cosine_similarity_values, \n",
    "    mean_spearman_r,\n",
    "    np.array(ml_accuracy[\"svm_test\"]),\n",
    "    np.array(ml_accuracy[\"knn_test\"])\n",
    "])\n",
    "\n",
    "corr_data = np.where(corr_data==0, 0.00000001, corr_data)\n",
    "mask = np.triu(np.ones_like(np.empty((len(corr_data), len(corr_data)))), 1)\n",
    "\n",
    "seaborn.heatmap(\n",
    "    np.round(np.corrcoef(corr_data), 2), \n",
    "    yticklabels=corr_data_labels, \n",
    "    xticklabels=corr_data_labels,\n",
    "    annot=True, \n",
    "    fmt=\".2f\",\n",
    "    mask=mask\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "corr_data = np.transpose(corr_data)\n",
    "\n",
    "seaborn.heatmap(\n",
    "    np.round(stats.spearmanr(corr_data)[0],2), \n",
    "    yticklabels=corr_data_labels, \n",
    "    xticklabels=corr_data_labels,\n",
    "    annot=True, \n",
    "    fmt=\".2f\",\n",
    "    mask=mask\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
