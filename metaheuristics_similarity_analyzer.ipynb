{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niapy.algorithms.basic import (\n",
    "    BatAlgorithm,\n",
    "    ParticleSwarmAlgorithm,\n",
    "    ParticleSwarmOptimization,\n",
    ")\n",
    "from msa.algorithms.fa import FireflyAlgorithm\n",
    "from niapy.problems.schwefel import Schwefel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn\n",
    "from scipy import spatial, stats\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from msa.util.optimization_data import SingleRunData\n",
    "from msa.util.pop_diversity_metrics import PopDiversityMetric\n",
    "from msa.tools.optimization_tools import optimization_runner\n",
    "\n",
    "from msa.util.constants import (\n",
    "    DATASET_PATH,\n",
    "    POP_SIZE,\n",
    "    MAX_EVALS,\n",
    "    OPTIMIZATION_PROBLEM,\n",
    "    POP_DIVERSITY_METRICS,\n",
    "    INDIV_DIVERSITY_METRICS,\n",
    ")\n",
    "\n",
    "BASE_PATH = \"./archive/target_performance_similarity/01-29_15.27.22_WVCPSO_Schwefel\"\n",
    "_DATASET_PATH = f\"{BASE_PATH}/dataset\"\n",
    "DATASET_PATH = f\"{_DATASET_PATH}/0_subset\"\n",
    "MSA_PATH = f\"{BASE_PATH}/msa_obj\"\n",
    "\n",
    "algorithms_to_plot = ['FA', 'WVCPSO', 'PSO', 'BA']\n",
    "\n",
    "execute_training = True\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "print(\"CPUs: \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_test_setting = True\n",
    "\n",
    "problem = OPTIMIZATION_PROBLEM\n",
    "\n",
    "if use_test_setting:\n",
    "    problem = Schwefel(dimension=20)\n",
    "    algorithms = [\n",
    "        #FireflyAlgorithm(population_size=POP_SIZE, alpha=0.15, beta0=0.4, gamma=0.04, theta=0.98),\n",
    "        FireflyAlgorithm(population_size=POP_SIZE, alpha=0.84464886, beta0=0.74171366, gamma=0.60686203, theta=0.97758844),\n",
    "        #FireflyAlgorithm(population_size=POP_SIZE, alpha=0.02, beta0=0.43, gamma=0.693, theta=0.962),\n",
    "        #ParticleSwarmAlgorithm(population_size=POP_SIZE, c1=1.14, c2=0.05, w=0.54),\n",
    "        #ParticleSwarmAlgorithm(population_size=POP_SIZE, c1=2.00417841, c2=0.70674774, w=0.82266951),\n",
    "        #ParticleSwarmAlgorithm(population_size=POP_SIZE, c1=1.95, c2=0.82, w=0.82),\n",
    "        #BatAlgorithm(population_size=POP_SIZE, loudness=1.0, pulse_rate=1.0, alpha=0.99, gamma=0.1)\n",
    "        #tools.algorithms.pso.ParticleSwarmAlgorithm(population_size=POP_SIZE, c1=1.95, c2=0.82, w=0.82),\n",
    "    ]\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    optimization_runner(\n",
    "        algorithm=algorithm,\n",
    "        problem=problem,\n",
    "        runs=1,\n",
    "        dataset_path=\"./dataset\",\n",
    "        pop_diversity_metrics=POP_DIVERSITY_METRICS,\n",
    "        indiv_diversity_metrics=INDIV_DIVERSITY_METRICS,\n",
    "        max_evals=MAX_EVALS,\n",
    "        run_index_seed=True,\n",
    "        keep_pop_data=False,\n",
    "        parallel_processing=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population diversity metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_metrics_list = [\n",
    "    PopDiversityMetric.PDC,\n",
    "    PopDiversityMetric.FDC,\n",
    "    PopDiversityMetric.PFSD,\n",
    "    PopDiversityMetric.PFM,\n",
    "]\n",
    "\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        runs = os.listdir(os.path.join(DATASET_PATH, algorithm, problem))\n",
    "        runs.sort()\n",
    "        run_path = os.path.join(DATASET_PATH, algorithm, problem, runs[0])\n",
    "        srd = SingleRunData.import_from_json(run_path)\n",
    "        pop_metrics = SingleRunData.import_from_json(run_path).get_pop_diversity_metrics_values(metrics=pop_metrics_list, minmax_scale=False, standard_scale=True)\n",
    "        ax = pop_metrics.plot(figsize=(15,7), fontsize=19, logy=False)\n",
    "        ax.set_title(label=\" \".join([f\"Populacijske metrike raznolikosti - {algorithm}\", problem]), fontdict={'fontsize':22}, pad=15)\n",
    "        ax.set_xlabel(xlabel=\"Iteracija\", fontdict={'fontsize':19}, labelpad=10)\n",
    "        ax.set_ylabel(ylabel=\"Vrednost\", fontdict={'fontsize':19}, labelpad=10)\n",
    "        ax.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_styles = ['-g', ':g', '--g', '-.g', '-b', ':b', '--b', '-.b']\n",
    "_line_styles = ['-g', '-b', '-r', '-k', ':g', ':b', ':r', ':k']\n",
    "style = {}\n",
    "pop_metrics_list = [\n",
    "    PopDiversityMetric.PDC,\n",
    "    PopDiversityMetric.FDC,\n",
    "    PopDiversityMetric.PFSD,\n",
    "    PopDiversityMetric.PFM,\n",
    "]\n",
    "style_idx = 0\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    if algorithm not in algorithms_to_plot:\n",
    "        continue\n",
    "    for idx, metric in enumerate(pop_metrics_list):\n",
    "        if idx > 3:\n",
    "            continue\n",
    "        style['_'.join([algorithm, metric.value])] = line_styles[style_idx]\n",
    "        style_idx += 1\n",
    "\n",
    "metrics_by_problem = {}\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    if algorithm not in algorithms_to_plot:\n",
    "        continue\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        runs = os.listdir(os.path.join(DATASET_PATH, algorithm, problem))\n",
    "        runs.sort()\n",
    "        run_path = os.path.join(DATASET_PATH, algorithm, problem, runs[0])\n",
    "        run = SingleRunData.import_from_json(run_path)\n",
    "        pop_metrics = run.get_pop_diversity_metrics_values(metrics=pop_metrics_list, minmax_scale=False)\n",
    "        \n",
    "        for metric in pop_metrics_list:\n",
    "            key = '_'.join([algorithm, metric.value])\n",
    "            if metric.value not in pop_metrics:\n",
    "                continue\n",
    "\n",
    "            if problem in metrics_by_problem:\n",
    "                metrics_by_problem[problem][key] = pop_metrics.get(metric.value).to_list()\n",
    "            else:\n",
    "                metric_values = {key: pop_metrics.get(metric.value).to_list()}\n",
    "                metrics_by_problem[problem] = metric_values\n",
    "\n",
    "            # scale fdc to [0, 1] for easier comparison on logy scale\n",
    "            if metric == PopDiversityMetric.FDC:\n",
    "                metrics_by_problem[problem][key] = sklearn.preprocessing.minmax_scale(\n",
    "                    metrics_by_problem[problem][key], feature_range=(0, 1)\n",
    "                )\n",
    "\n",
    "for problem in metrics_by_problem:\n",
    "    metrics = metrics_by_problem[problem]\n",
    "    df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "    ax = df_metrics.plot(style=style, figsize=(25, 7), logy=True, fontsize=15)\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_title(label=problem, fontdict={'fontsize':24})\n",
    "    ax.set_xlabel(xlabel=\"Iterations\", fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "\n",
    "    line_styles = ['-g', ':g', '--g', '-.g', '-b', ':b', '--b', '-.b']\n",
    "    _line_styles = ['-g', '-b', '-r', '-k', ':g', ':b', ':r', ':k']\n",
    "    style = {}\n",
    "    pop_metrics_list = [\n",
    "        PopDiversityMetric.PDC,\n",
    "        PopDiversityMetric.FDC,\n",
    "        PopDiversityMetric.PFSD,\n",
    "        PopDiversityMetric.PFM,\n",
    "    ]\n",
    "    style_idx = 0\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for idx, metric in enumerate(pop_metrics_list):\n",
    "            if idx > 3:\n",
    "                continue\n",
    "            style['_'.join([algorithm, metric.value])] = line_styles[style_idx]\n",
    "            style_idx += 1\n",
    "\n",
    "    metrics_by_problem = {}\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            run_path = os.path.join(dataset_path, subset, algorithm, problem, runs[0])\n",
    "            run = SingleRunData.import_from_json(run_path)\n",
    "            pop_metrics = run.get_pop_diversity_metrics_values(metrics=pop_metrics_list, minmax_scale=False)\n",
    "            \n",
    "            for metric in pop_metrics_list:\n",
    "                key = '_'.join([algorithm, metric.value])\n",
    "                if metric.value not in pop_metrics:\n",
    "                    continue\n",
    "\n",
    "                if problem in metrics_by_problem:\n",
    "                    metrics_by_problem[problem][key] = pop_metrics.get(metric.value).to_list()\n",
    "                else:\n",
    "                    metric_values = {key: pop_metrics.get(metric.value).to_list()}\n",
    "                    metrics_by_problem[problem] = metric_values\n",
    "\n",
    "                # scale fdc to [0, 1] for easier comparison on logy scale\n",
    "                if metric == PopDiversityMetric.FDC:\n",
    "                    metrics_by_problem[problem][key] = sklearn.preprocessing.minmax_scale(\n",
    "                        metrics_by_problem[problem][key], feature_range=(0, 1)\n",
    "                    )\n",
    "\n",
    "    for problem in metrics_by_problem:\n",
    "        metrics = metrics_by_problem[problem]\n",
    "        df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "        ax = df_metrics.plot(style=style, figsize=(25, 7), logy=True, fontsize=15)\n",
    "        ax.legend(fontsize=15)\n",
    "        ax.set_title(label=subset, fontdict={'fontsize':24})\n",
    "        ax.set_xlabel(xlabel=\"Iterations\", fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average population diversity metrics for all runs by subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "\n",
    "    line_styles = ['-g', ':g', '--g', '-.g', '-b', ':b', '--b', '-.b']\n",
    "    _line_styles = ['-g', '-b', '-r', '-k', ':g', ':b', ':r', ':k']\n",
    "    style = {}\n",
    "    pop_metrics_list = [\n",
    "        PopDiversityMetric.PDC,\n",
    "        PopDiversityMetric.FDC,\n",
    "        PopDiversityMetric.PFSD,\n",
    "        PopDiversityMetric.PFM,\n",
    "    ]\n",
    "    style_idx = 0\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for idx, metric in enumerate(pop_metrics_list):\n",
    "            if idx > 3:\n",
    "                continue\n",
    "            style['_'.join([algorithm, metric.value])] = line_styles[style_idx]\n",
    "            style_idx += 1\n",
    "\n",
    "    metrics_by_problem = {}\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            for run_file_name in runs:\n",
    "                run_path = os.path.join(dataset_path, subset, algorithm, problem, run_file_name)\n",
    "                run = SingleRunData.import_from_json(run_path)\n",
    "                pop_metrics = run.get_pop_diversity_metrics_values(metrics=pop_metrics_list, minmax_scale=False)\n",
    "                \n",
    "                for metric in pop_metrics_list:\n",
    "                    key = '_'.join([algorithm, metric.value])\n",
    "                    if metric.value not in pop_metrics:\n",
    "                        continue\n",
    "\n",
    "                    if problem in metrics_by_problem:\n",
    "                        if key in metrics_by_problem[problem]:\n",
    "                            sum_of_metrics = np.add(\n",
    "                                metrics_by_problem[problem][key], pop_metrics.get(metric.value).to_numpy() / len(runs)\n",
    "                            )\n",
    "                            metrics_by_problem[problem][key] = sum_of_metrics\n",
    "                        else:\n",
    "                            metrics_by_problem[problem][key] = pop_metrics.get(metric.value).to_numpy() / len(runs)\n",
    "                    else:\n",
    "                        metric_values = {key: pop_metrics.get(metric.value).to_numpy() / len(runs)}\n",
    "                        metrics_by_problem[problem] = metric_values\n",
    "\n",
    "                    # scale fdc to [0, 1] for easier comparison on logy scale\n",
    "                    \"\"\"\n",
    "                    if metric == PopDiversityMetric.FDC:\n",
    "                        metrics_by_problem[problem][key] = sklearn.preprocessing.minmax_scale(\n",
    "                            metrics_by_problem[problem][key], feature_range=(0, 1)\n",
    "                        )\n",
    "                    \"\"\"\n",
    "\n",
    "    for problem in metrics_by_problem:\n",
    "        metrics = metrics_by_problem[problem]\n",
    "        df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "        ax = df_metrics.plot(style=style, figsize=(25, 7), logy=True, fontsize=15)\n",
    "        ax.legend(fontsize=15)\n",
    "        ax.set_title(label=subset, fontdict={'fontsize':24})\n",
    "        ax.set_xlabel(xlabel=\"Iterations\", fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual diversity metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_problem = {}\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    if algorithm not in [\"WVCPSO\"]:#algorithms_to_plot:\n",
    "        continue\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        runs = os.listdir(os.path.join(DATASET_PATH, algorithm, problem))\n",
    "        runs.sort()\n",
    "        run_path = os.path.join(DATASET_PATH, algorithm, problem, runs[0])\n",
    "        run = SingleRunData.import_from_json(run_path)\n",
    "        indiv_metrics = run.get_indiv_diversity_metrics_values(minmax_scale=False, standard_scale=True)\n",
    "        for metric in INDIV_DIVERSITY_METRICS:\n",
    "            key = '_'.join([algorithm, metric.value])\n",
    "            if problem in metrics_by_problem:\n",
    "                metrics_by_problem[problem][key] = indiv_metrics.get(metric.value).to_list()\n",
    "            else:\n",
    "                metric_values = {key: indiv_metrics.get(metric.value).to_list()}\n",
    "                metrics_by_problem[problem] = metric_values\n",
    "        \n",
    "\n",
    "for problem in metrics_by_problem:\n",
    "    metrics = metrics_by_problem[problem]\n",
    "    df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(INDIV_DIVERSITY_METRICS))\n",
    "    fig.subplots_adjust(wspace=0.7, top=0.825, bottom=0)\n",
    "    #fig.suptitle(problem, fontsize=20)\n",
    "    fig.suptitle(\"Individualne metrike raznolikosti - Schwefel\", fontsize=22)\n",
    "\n",
    "    for idx, metric in enumerate(INDIV_DIVERSITY_METRICS):\n",
    "        df_metric = df_metrics.filter(regex=metric.value)\n",
    "        df_metric.columns = df_metric.columns.str.replace(\"WVCPSO\" + '_'+metric.value, 'PSO')\n",
    "        ax = df_metric.plot(ax=axes[idx], kind=\"box\", figsize=(15, 5), logy=False, fontsize=19)\n",
    "        ax.margins(x=0)\n",
    "        ax.set_title(label=metric.name, fontdict={'fontsize':19}, pad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "\n",
    "    metrics_by_problem = {}\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            run_path = os.path.join(dataset_path, subset, algorithm, problem, runs[0])\n",
    "            run = SingleRunData.import_from_json(run_path)\n",
    "            indiv_metrics = run.get_indiv_diversity_metrics_values(minmax_scale=False, standard_scale=True)\n",
    "            for metric in INDIV_DIVERSITY_METRICS:\n",
    "                key = '_'.join([algorithm, metric.value])\n",
    "                if problem in metrics_by_problem:\n",
    "                    metrics_by_problem[problem][key] = indiv_metrics.get(metric.value).to_list()\n",
    "                else:\n",
    "                    metric_values = {key: indiv_metrics.get(metric.value).to_list()}\n",
    "                    metrics_by_problem[problem] = metric_values\n",
    "            \n",
    "\n",
    "    for problem in metrics_by_problem:\n",
    "        metrics = metrics_by_problem[problem]\n",
    "        df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(INDIV_DIVERSITY_METRICS))\n",
    "        fig.suptitle(f\"{problem} {subset}\", fontsize=23)\n",
    "        fig.subplots_adjust(wspace=0.4)\n",
    "\n",
    "        for idx, metric in enumerate(INDIV_DIVERSITY_METRICS):\n",
    "            df_metric = df_metrics.filter(regex=metric.value)\n",
    "            df_metric.columns = df_metric.columns.str.replace('_'+metric.value, '')\n",
    "            ax = df_metric.plot(ax=axes[idx], kind=\"box\", figsize=(25, 6), logy=False, fontsize=15)\n",
    "            ax.set_title(label=metric.name, fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average of individual diversity metrics for all runs by subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = _DATASET_PATH\n",
    "\n",
    "subsets = os.listdir(dataset_path)\n",
    "\n",
    "for idx in range(len(subsets)):\n",
    "    subset = f\"{idx}_subset\"\n",
    "\n",
    "    metrics_by_problem = {}\n",
    "    for algorithm in os.listdir(os.path.join(dataset_path, subset)):\n",
    "        if algorithm not in algorithms_to_plot:\n",
    "            continue\n",
    "        for problem in os.listdir(os.path.join(dataset_path, subset, algorithm)):\n",
    "            runs = os.listdir(os.path.join(dataset_path, subset, algorithm, problem))\n",
    "            runs.sort()\n",
    "            for run_file_name in runs:\n",
    "                run_path = os.path.join(dataset_path, subset, algorithm, problem, runs[0])\n",
    "                run = SingleRunData.import_from_json(run_path)\n",
    "                indiv_metrics = run.get_indiv_diversity_metrics_values(minmax_scale=False)\n",
    "                for metric in INDIV_DIVERSITY_METRICS:\n",
    "                    key = '_'.join([algorithm, metric.value])\n",
    "                    if problem in metrics_by_problem:\n",
    "                        if key in metrics_by_problem[problem]:\n",
    "                            sum_of_metrics = np.add(\n",
    "                                metrics_by_problem[problem][key], indiv_metrics.get(metric.value).to_numpy() / len(runs)\n",
    "                            )\n",
    "                            metrics_by_problem[problem][key] = sum_of_metrics\n",
    "                        else:    \n",
    "                            metrics_by_problem[problem][key] = indiv_metrics.get(metric.value).to_numpy() / len(runs)\n",
    "                    else:\n",
    "                        metric_values = {key: indiv_metrics.get(metric.value).to_numpy() / len(runs)}\n",
    "                        metrics_by_problem[problem] = metric_values\n",
    "            \n",
    "\n",
    "    for problem in metrics_by_problem:\n",
    "        metrics = metrics_by_problem[problem]\n",
    "        df_metrics = pd.DataFrame.from_dict(metrics)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(INDIV_DIVERSITY_METRICS))\n",
    "        fig.suptitle(f\"{problem} {subset}\", fontsize=23)\n",
    "        fig.subplots_adjust(wspace=0.4)\n",
    "\n",
    "        for idx, metric in enumerate(INDIV_DIVERSITY_METRICS):\n",
    "            df_metric = df_metrics.filter(regex=metric.value)\n",
    "            df_metric.columns = df_metric.columns.str.replace('_'+metric.value, '')\n",
    "            ax = df_metric.plot(ax=axes[idx], kind=\"box\", figsize=(25, 6), logy=True, fontsize=15)\n",
    "            ax.set_title(label=metric.name, fontdict={'fontsize':20})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
