{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "from niapy.algorithms.basic import (\n",
    "    BatAlgorithm,\n",
    "    FireflyAlgorithm,\n",
    "    ParticleSwarmOptimization,\n",
    ")\n",
    "from niapy.algorithms import Algorithm\n",
    "from niapy.problems import Problem\n",
    "from niapy.problems.schwefel import Schwefel\n",
    "from niapy.task import Task\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import sklearn.model_selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from util.optimization_data import SingleRunData, PopulationData\n",
    "from util.pop_diversity_metrics import PopDiversityMetric\n",
    "from util.constants import RNG_SEED, DATASET_PATH, BATCH_SIZE, EPOCHS, POP_SIZE, MAX_ITER, NUM_RUNS\n",
    "\n",
    "execute_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(algorithm: Algorithm, task: Task, single_run_data: SingleRunData):\n",
    "    r\"\"\"An adaptation of NiaPy Algorithm run method.\n",
    "\n",
    "    Args:\n",
    "        algorithm (Algorithm): Algorithm.\n",
    "        task (Task): Task with pre configured parameters.\n",
    "        single_run_data (SingleRunData): Instance for archiving optimization results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        algorithm.callbacks.before_run()\n",
    "        algorithm.rng = default_rng(seed=RNG_SEED)\n",
    "        pop, fpop, params = algorithm.init_population(task)\n",
    "        # reset seed to random\n",
    "        algorithm.rng = default_rng()\n",
    "        xb, fxb = algorithm.get_best(pop, fpop)\n",
    "        while not task.stopping_condition():\n",
    "            # save population data\n",
    "            pop_data = PopulationData(\n",
    "                population=np.array(pop), population_fitness=np.array(fpop)\n",
    "            )\n",
    "            pop_data.calculate_metrics(\n",
    "                [\n",
    "                    PopDiversityMetric.PDC,\n",
    "                    PopDiversityMetric.PED,\n",
    "                    PopDiversityMetric.PMD,\n",
    "                    PopDiversityMetric.AAD,\n",
    "                    PopDiversityMetric.PDI,\n",
    "                    PopDiversityMetric.PFSD,\n",
    "                    PopDiversityMetric.PFMea,\n",
    "                    PopDiversityMetric.PFMed,\n",
    "                ],\n",
    "                task.problem,\n",
    "            )\n",
    "            single_run_data.add_population(pop_data)\n",
    "            algorithm.callbacks.before_iteration(pop, fpop, xb, fxb, **params)\n",
    "            pop, fpop, xb, fxb, params = algorithm.run_iteration(\n",
    "                task, pop, fpop, xb, fxb, **params\n",
    "            )\n",
    "\n",
    "            algorithm.callbacks.after_iteration(pop, fpop, xb, fxb, **params)\n",
    "            task.next_iter()\n",
    "        algorithm.callbacks.after_run()\n",
    "        single_run_data.calculate_indiv_diversity_metrics()\n",
    "        return xb, fxb * task.optimization_type.value\n",
    "    except BaseException as e:\n",
    "        if (\n",
    "            threading.current_thread() is threading.main_thread()\n",
    "            and multiprocessing.current_process().name == \"MainProcess\"\n",
    "        ):\n",
    "            raise e\n",
    "        algorithm.exception = e\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_runner(\n",
    "    algorithm: Algorithm, problem: Problem, max_iter: int, runs: int\n",
    "):\n",
    "    r\"\"\"An adaptation of NiaPy ALgorithm run method.\n",
    "\n",
    "    Args:\n",
    "        algorithm (Algorithm): Algorithm.\n",
    "        problem (Problem): Optimization problem.\n",
    "        max_iter (int): Optimization stopping condition.\n",
    "        runs (int): Number of runs to execute.\n",
    "    \"\"\"\n",
    "    for r in tqdm(range(runs)):\n",
    "        task = Task(problem, max_iters=max_iter)\n",
    "\n",
    "        single_run_data = SingleRunData(\n",
    "            algorithm_name=algorithm.Name,\n",
    "            algorithm_parameters=algorithm.get_parameters(),\n",
    "            problem_name=problem.name(),\n",
    "            max_iters=max_iter,\n",
    "        )\n",
    "\n",
    "        results = optimization(algorithm, task, single_run_data)\n",
    "        single_run_data.export_to_json(\n",
    "            os.path.join(\n",
    "                DATASET_PATH, algorithm.Name[0], problem.name(), f\"run_{r:05d}.json\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in [BatAlgorithm(population_size=POP_SIZE), FireflyAlgorithm(population_size=POP_SIZE), ParticleSwarmOptimization(population_size=POP_SIZE, c1=1.0, c2=1.0)]:\n",
    "    problem = Schwefel()\n",
    "    optimization_runner(algorithm, problem, MAX_ITER, NUM_RUNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        runs = os.listdir(os.path.join(DATASET_PATH, algorithm, problem))\n",
    "        run_path = os.path.join(DATASET_PATH, algorithm, problem, runs[0])\n",
    "        pop_metrics = SingleRunData.import_from_json(run_path).get_pop_diversity_metrics_values(normalize=True)\n",
    "        pop_metrics.plot(title=\" \".join([algorithm, problem]), figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path_list, classes) -> None:\n",
    "        super().__init__()\n",
    "        self.data_path_list = data_path_list\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        run = SingleRunData.import_from_json(self.data_path_list[index])\n",
    "        pop_metrics = run.get_pop_diversity_metrics_values(normalize=True).to_numpy()\n",
    "        indiv_metrics = run.get_indiv_diversity_metrics_values(\n",
    "            normalize=True\n",
    "        ).to_numpy()\n",
    "\n",
    "        pca = PCA(n_components=indiv_metrics.shape[1])\n",
    "        principal_components = pca.fit_transform(indiv_metrics).flatten()\n",
    "        variance = pca.explained_variance_ratio_\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(pop_metrics).float(),\n",
    "            torch.from_numpy(principal_components).float(),\n",
    "            torch.tensor(self.classes.index(run.algorithm_name[0])),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, aux_input_dim, num_classes, hidden_dim=256, num_layers=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.8\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim + aux_input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, aux):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        features_0 = lstm_out[:, -1]\n",
    "        features = torch.concat([features_0, aux], dim=1)\n",
    "        out = self.fc(features)\n",
    "\n",
    "        return features, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = []\n",
    "classes = []\n",
    "for algorithm in os.listdir(DATASET_PATH):\n",
    "    classes.append(algorithm)\n",
    "    for problem in os.listdir(os.path.join(DATASET_PATH, algorithm)):\n",
    "        for run in os.listdir(os.path.join(DATASET_PATH, algorithm, problem)):\n",
    "            dataset_paths.append(os.path.join(DATASET_PATH, algorithm, problem, run))\n",
    "\n",
    "x_train, x_test = sklearn.model_selection.train_test_split(dataset_paths, test_size=0.2, shuffle=True, random_state=RNG_SEED)\n",
    "x_train, x_val = sklearn.model_selection.train_test_split(x_train, test_size=0.25, shuffle=True, random_state=RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_generator(x_train, classes)\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "val_dataset = data_generator(x_val, classes)\n",
    "val_data_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "test_dataset = data_generator(x_test, classes)\n",
    "test_data_loader = data.DataLoader(test_dataset, batch_size=1, shuffle=True, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "pop_features, indiv_features, target = next(iter(train_data_loader))\n",
    "print(np.shape(pop_features))\n",
    "print(np.shape(indiv_features))\n",
    "print(np.shape(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(model, train_data_loader, val_data_loader, epochs, loss_fn, optimizer, device, batch_size, model_file_name):\n",
    "    loss_values = []\n",
    "    val_loss_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0.0\n",
    "        val_loss_sum = 0.0\n",
    "            \n",
    "        model.train()\n",
    "        for batch in train_data_loader:\n",
    "            pop_features, indiv_features, target = batch\n",
    "            \n",
    "            target = target.to(device)\n",
    "            pop_features = pop_features.to(device)\n",
    "            indiv_features = indiv_features.to(device)\n",
    "\n",
    "            _, pred = model(pop_features, indiv_features)\n",
    "            loss = loss_fn(pred, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_data_loader:\n",
    "                pop_features, indiv_features, target = batch\n",
    "            \n",
    "                target = target.to(device)\n",
    "                pop_features = pop_features.to(device)\n",
    "                indiv_features = indiv_features.to(device)\n",
    "\n",
    "                _, pred = model(pop_features, indiv_features)\n",
    "                loss = loss_fn(pred, target)\n",
    "\n",
    "                val_loss_sum += loss.item()\n",
    "\n",
    "        loss_values.append(loss_sum/batch_size)\n",
    "        val_loss_values.append(val_loss_sum/batch_size)\n",
    "        print(f\"epoch: {epoch + 1}, loss: {loss_sum/batch_size :.10f}, val_loss: {val_loss_sum/batch_size :.10f}\")\n",
    "\n",
    "        torch.save(model, model_file_name)\n",
    "\n",
    "    x = [*range(1, epochs+1)]\n",
    "    plt.plot(x, loss_values, label=\"train loss\")\n",
    "    plt.plot(x, val_loss_values, label=\"val loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(\"loss_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn(model, test_data_loader, device, classes):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = []\n",
    "    y_target = []\n",
    "\n",
    "    for batch in test_data_loader:\n",
    "        pop_features, indiv_features, target = batch\n",
    "            \n",
    "        target = target.to(device)\n",
    "        pop_features = pop_features.to(device)\n",
    "        indiv_features = indiv_features.to(device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, pred = model(pop_features, indiv_features)\n",
    "            \n",
    "            y_pred.append(torch.argmax(pred).cpu().numpy())\n",
    "            y_target.append(target.cpu().numpy()[0])\n",
    "\n",
    "    print(classification_report(y_target, y_pred))\n",
    "    cf_matrix = confusion_matrix(y_target, y_pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix, index = [i for i in classes], columns = [i for i in classes])\n",
    "    plt.figure(figsize = (12,7))\n",
    "    sb.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "print(\"CPUs: \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_features, indiv_features, target = next(iter(train_data_loader))\n",
    "model = LSTM(np.shape(pop_features)[2], np.shape(indiv_features)[1], len(classes))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_file_name = f\"lstm_model.pt\"\n",
    "\n",
    "if execute_training:\n",
    "    model.to(device)\n",
    "    nn_train(model, train_data_loader, val_data_loader, EPOCHS, loss_fn, optimizer, device, BATCH_SIZE, model_file_name)\n",
    "    torch.save(model, model_file_name)\n",
    "else:\n",
    "    model = torch.load(model_file_name, map_location=torch.device(device))\n",
    "    model.to(device)\n",
    "    loss_plot = np.asarray(Image.open('loss_plot.png'))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nn(model, test_data_loader, device, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
